<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Alephn&#39;s Blog</title>
  
  <subtitle>Stay hungry, stay foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-16T01:45:08.492Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Alephn</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>STAT157 L10-L11</title>
    <link href="http://yoursite.com/2019/08/14/STAT157-L10-L11/"/>
    <id>http://yoursite.com/2019/08/14/STAT157-L10-L11/</id>
    <published>2019-08-14T02:24:48.000Z</published>
    <updated>2019-08-16T01:45:08.492Z</updated>
    
    <content type="html"><![CDATA[<h1 id="STAT157-L10-L11笔记"><a href="#STAT157-L10-L11笔记" class="headerlink" title="STAT157 L10-L11笔记"></a>STAT157 L10-L11笔记</h1><p>关于课程<a href="https://courses.d2l.ai/berkeley-stat-157/index.html" target="_blank" rel="noopener">STAT157</a>的学习笔记</p><p>学习时间2019/8/14</p> <a id="more"></a><h2 id="Layers-Parameters-GPUs"><a href="#Layers-Parameters-GPUs" class="headerlink" title="Layers, Parameters, GPUs"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/layers.html#" target="_blank" rel="noopener">Layers, Parameters, GPUs</a></h2><p>这一节主要讲解了一些利用mxnet编程的知识，也可以参考<a href="http://beta.mxnet.io/guide/packages/gluon/index.html" target="_blank" rel="noopener">MXnet tutorial</a></p><h2 id="Convolutional-Networks"><a href="#Convolutional-Networks" class="headerlink" title="Convolutional Networks"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/convnet.html#" target="_blank" rel="noopener">Convolutional Networks</a></h2><h3 id="Convolutions"><a href="#Convolutions" class="headerlink" title="Convolutions"></a>Convolutions</h3><ul><li><p><a href="https://en.wikipedia.org/wiki/Convolution" target="_blank" rel="noopener">Definition of Convolution</a></p><script type="math/tex; mode=display">\begin{align}(f*g)(x)&=\int_{-\infty}^{\infty}f(\tau)\cdot g(x-\tau)d\tau\\(f*g)(x)&=\sum_{-\infty}^{\infty}f(\tau)\cdot g(x-\tau)\end{align}</script><p>上面是连续以及离散的情况下卷积的定义，需要注意的是积分变量是$\tau$而不是$x$，关于如何理解卷积，可以参考<a href="https://www.zhihu.com/question/22298352" target="_blank" rel="noopener">如何通俗易懂地解释卷积</a>和<a href="http://mengqi92.github.io/2015/10/06/convolution/" target="_blank" rel="noopener">我对卷积的理解</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/Cross-correlation" target="_blank" rel="noopener">Definition of Cross-correlation</a></p><script type="math/tex; mode=display">\begin{align}(f\star g)(x)&=\int_{-\infty}^{\infty}f(\tau)\cdot g(x+\tau)d\tau\\(f\star g)(x)&=\sum_{-\infty}^{\infty}f(\tau)\cdot g(x+\tau)\end{align}</script><p>这里仅仅考虑实数，不涉及复数</p></li><li><p>Rethinking dense layer</p><script type="math/tex; mode=display">h_{i,j}=\sum_{k, l}W_{i,j,k,l}X_{k,l}</script><p>当处理图片时，假设输入的原始状态是二维的，为了保持隐层输出一致，我们可以利用四维的张量来进行变换，具体的公式如上，进行变量代换：</p><script type="math/tex; mode=display">\begin{align}&h_{i,j}=\sum_{a,b}V_{i,j,a,b}X_{i+a,j+b}\\&W_{i,j,i+a,j+b}=V_{i,j,a,b}\end{align}</script><p>假设$V$与$i,j$独立，即$V_{i,j,a,b}=V_{a,b}$，那么:</p><script type="math/tex; mode=display">h_{i,j}=\sum_{a,b}V_{a,b}X_{i+a,j+b}</script><p>可以看到$h$是$V,X$的互相关函数</p><p>当对$X$进行变换得到$h_{i,j}$时，我们关注的区域不会离$X_{i,j}$太远，于是有：</p><script type="math/tex; mode=display">h_{i,j}=\sum_{a=-\Delta}^\Delta \sum_{b=-\Delta}^\Delta V_{a,b}X_{i+a,j+b}</script><p>在$a,b$超出范围时令$V_{a,b}=0$</p></li><li><p>Convolution layer vs. Cross-correlation layer</p><p><img src="/2019/08/14/STAT157-L10-L11/layer.png" alt="layer"></p><p>相比于卷积运算，互相关更能发挥出计算机的cache机制优势</p></li></ul><h3 id="Padding-and-Stride"><a href="#Padding-and-Stride" class="headerlink" title="Padding and Stride"></a>Padding and Stride</h3><ul><li><p>padding：假设卷积核的尺寸为$k_h\times k_w$，输入的尺寸为$n_h\times n_w$，填充$p_h$行，$p_w$列，那么输出的尺寸为：</p><script type="math/tex; mode=display">(n_h-k_h+p_h+1)\times (n_w-k_w+p_w+1)</script><p>一般选择$p_h=k_h-1, p_w=k_w-1$，$k$为奇数，两边填充数为$\frac {p}{2}$,$k$为偶数，一边填充$\lceil \frac p2\rceil$，一边填充$\lfloor\frac p2\rfloor$</p></li><li><p>strides：假设步长为$s_h,s_w$，那么输出尺寸为：</p><script type="math/tex; mode=display">\lfloor (n_h-k_h+p_h+1)/s_h\rfloor\times \lfloor(n_w-k_w+p_w+1)/s_w\rfloor</script></li></ul><h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/08/14/STAT157-L10-L11/channel.png" alt="channel" title="">                </div>                <div class="image-caption">channel</div>            </figure></li><li><p>$c$表示channel数，$B$表示bias</p><ul><li><p>Multiple input channels, single output channel</p><p>$X: c_i\times n_h\times n_w$</p><p>$W: c_i\times k_h\times k_w$</p><p>$Y:  m_h\times m_w$</p><p>$Y=\sum_{j=0}^{c_i}X_j\star W_j+B_j$</p></li><li><p>Multiple input&amp;output channels</p><p>$X: c_i\times n_h\times n_w$</p><p>$W: c_o\times  c_i\times k_h\times k_w$</p><p>$Y: c_o\times m_h\times m_w$</p><p>$Y_i=\sum_{j=0}^{c_i}X_j\star W_{i,j}+B_{i,j}$</p></li></ul></li></ul><h3 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h3><ul><li><p><a href="https://www.quora.com/What-is-the-motivation-for-pooling-in-convolutional-neural-networks-CNN" target="_blank" rel="noopener">Why we need pooling?</a></p><blockquote><p><strong>Rotational/Position Invariance Feature Extraction :</strong> Pooling can also be used for extracting rotational and position invariant feature. Consider the same example of using pooling of size 5x5. Pooling extracts the max value from the given 5x5 region. Basically extract the dominant feature value (max value) from the given region irrespective of the position of the feature value. The max value would be from any position inside the region. Pooling does not capture the position of the max value thus provides rotational/positional invariant feature extraction.</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;STAT157-L10-L11笔记&quot;&gt;&lt;a href=&quot;#STAT157-L10-L11笔记&quot; class=&quot;headerlink&quot; title=&quot;STAT157 L10-L11笔记&quot;&gt;&lt;/a&gt;STAT157 L10-L11笔记&lt;/h1&gt;&lt;p&gt;关于课程&lt;a href=&quot;https://courses.d2l.ai/berkeley-stat-157/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;STAT157&lt;/a&gt;的学习笔记&lt;/p&gt;
&lt;p&gt;学习时间2019/8/14&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine lerning" scheme="http://yoursite.com/tags/machine-lerning/"/>
    
      <category term="STAT157" scheme="http://yoursite.com/tags/STAT157/"/>
    
  </entry>
  
  <entry>
    <title>STAT157 L8-L9</title>
    <link href="http://yoursite.com/2019/08/11/STAT157-L8-L9/"/>
    <id>http://yoursite.com/2019/08/11/STAT157-L8-L9/</id>
    <published>2019-08-11T02:59:29.000Z</published>
    <updated>2019-08-12T04:56:10.417Z</updated>
    
    <content type="html"><![CDATA[<h1 id="STAT157-L8-L9笔记"><a href="#STAT157-L8-L9笔记" class="headerlink" title="STAT157 L8-L9笔记"></a>STAT157 L8-L9笔记</h1><p>关于课程<a href="https://courses.d2l.ai/berkeley-stat-157/index.html" target="_blank" rel="noopener">STAT157</a>的学习笔记</p><p>学习时间2019/8/10</p> <a id="more"></a><h2 id="Numerical-Stability-Hardware"><a href="#Numerical-Stability-Hardware" class="headerlink" title="Numerical Stability, Hardware"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/dropout.html#" target="_blank" rel="noopener">Numerical Stability, Hardware</a></h2><h3 id="Numerical-Stability"><a href="#Numerical-Stability" class="headerlink" title="Numerical Stability"></a>Numerical Stability</h3><ul><li><p>在训练神经网络的过程中，需要注意数值稳定性问题，如梯度爆炸和梯度消失。</p></li><li><p>以MLP为例，假设前向反馈过程如下：</p><script type="math/tex; mode=display">\begin{align}Z_1&=X*W_1+b_1\\A_1&=\sigma_1(Z_1)\\&\cdots\\Z_i&=A_i*W_{i-1}+b_i\\A_i&=\sigma_i(Z_i)\\&\cdots\\Z_n&=A_n*W_{n-1}+b_{n}\\A_n&=\sigma_n(Z_n)\end{align}</script><p>损失函数为$l=\zeta(A_n, Y)$，将$\frac{\partial l}{\partial p}$记做$dp$，那么有：</p><script type="math/tex; mode=display">\begin{align}dA_n&=dA_n\\dZ_n&=dA_n\cdot\dot{\sigma_n}(Z_n)\\dA_{n-1}&=dZ_n*W_n.T\\&\cdots\\dA_i&=dZ_{i+1}*W_{i+1}.T\\dZ_i&=dA_i\cdot\dot{\sigma_i}(Z_i)\\\end{align}</script><p>根据上面的偏导数计算公式，可以得到:</p><script type="math/tex; mode=display">\begin{align}dW_i&=A_{i-1}.T*dZ_i\\&=A_{i-1}.T*(dZ_{i+1}*W_{i+1}.T)\cdot \dot{\sigma}_i(Z_i)\\&=\cdots\\&=A_{i-1}.T*dA_n\cdot \dot{\sigma}_n(Z_n)*W_n.T\cdots \dot{\sigma}_{i+1}(Z_{i+1})*W_{i+1}.T\cdot \dot{\sigma}_{i}(Z_i)\end{align}</script><p>参考<a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" target="_blank" rel="noopener">The vanishing gradient problem</a>，如果选择sigmoid函数，$\dot{\sigma}$小于1/4，$||W||$小于1，因此当隐藏层数增加，会出现梯度消失，当使用relu函数时，若权重初始化较大，则会出现梯度爆炸。</p><p>（这里我查找了一些资料，但是感觉都很粗略，并没有找到细致的证明）</p></li></ul><h3 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h3><ul><li><p>为了解决上面的问题，可以通过选择初始化的方法（缓解问题），可以参考<a href="http://alephn.xin/2019/07/28/总结—2019-7-26/" target="_blank" rel="noopener">总结—2019/7/27</a></p></li><li><p>之前关于激活函数对初始化的影响不是很理解，课程中对此给出了解释</p><p><img src="/2019/08/11/STAT157-L8-L9/激活函数.png" alt="激活函数"></p><p>利用泰勒展开就可以大致了解如何选择激活函数</p></li></ul><h2 id="Machine-Learning-Problems-and-Statistical-Environment"><a href="#Machine-Learning-Problems-and-Statistical-Environment" class="headerlink" title="Machine Learning Problems and Statistical Environment"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/environment.html#" target="_blank" rel="noopener">Machine Learning Problems and Statistical Environment</a></h2><h3 id="Distribution-Shift"><a href="#Distribution-Shift" class="headerlink" title="Distribution Shift"></a>Distribution Shift</h3><h4 id="Covariate-Shift"><a href="#Covariate-Shift" class="headerlink" title="Covariate Shift"></a>Covariate Shift</h4><ul><li><blockquote><p>Here we assume that although the distribution of inputs may change over time, the labeling function, i.e., the conditional distribution $p(y|x)$ does not change.</p></blockquote></li><li><p>上面的定义来自<a href="http://d2l.ai/chapter_multilayer-perceptrons/environment.html" target="_blank" rel="noopener">d2l</a>，下面来介绍一个比较具体的例子进行解释(来自<a href="https://zhuanlan.zhihu.com/p/26352087" target="_blank" rel="noopener">基于样例的迁移学习——Covariate Shift——原始文章解读</a>)</p><script type="math/tex; mode=display">\begin{align}train\ data\ distribution&:x\sim N(0, 1)\\test\ data\ distribution&:x\sim N(0.5, 0.25)\\conditional\ distribution&:y|x\sim N(-x+x^3, 0.1^2)\end{align}</script></li><li><p>在<a href="http://d2l.ai/chapter_multilayer-perceptrons/environment.html" target="_blank" rel="noopener">d2l</a>中，有提到一般假定Covariate shift的条件，即输入导致输出</p><blockquote><p>when we believe x causes y, covariate shift is usually the right assumption to be working with.</p></blockquote></li></ul><h4 id="Label-Shift"><a href="#Label-Shift" class="headerlink" title="Label Shift"></a>Label Shift</h4><ul><li><blockquote><p>The converse problem emerges when we believe that what drives the shift is a change in the marginal distribution over the labels p(y) but that the class-conditional distributions are invariant $p(x|y)$.</p></blockquote></li><li><p>在<a href="http://d2l.ai/chapter_multilayer-perceptrons/environment.html" target="_blank" rel="noopener">d2l</a>中，有提到一般假定label shift的条件，即输出导致输入，例如在医疗影像中判断病症，认为病症是引起特定影像的原因</p><blockquote><p>Label shift is a reasonable assumption to make when we believe that y causes x</p></blockquote></li></ul><h4 id="Concept-Shift"><a href="#Concept-Shift" class="headerlink" title="Concept Shift"></a>Concept Shift</h4><ul><li><blockquote><p>One more related problem arises in concept shift, the situation in which the very label definitions change.</p></blockquote></li><li><p>例如软饮料在不同地区的称呼不同</p></li></ul><h4 id="Covariate-Shift-Correction"><a href="#Covariate-Shift-Correction" class="headerlink" title="Covariate Shift Correction"></a>Covariate Shift Correction</h4><ul><li><p>假设测试集$x\sim q(x)$，训练集$x\sim p(x)$，真实的数据与label联合分布为$(x, y)\sim p(x, y)=q(x)\cdot p(y|x)$，在训练集中我们最小化的是</p><script type="math/tex; mode=display">minimize_w\frac1m\sum_{i=1}^ml(f(x_i,w), y_i)+ some penalty(w)</script><p>或者用积分的形式：</p><script type="math/tex; mode=display">minimize_w\int p(x)dx\int l(f(x,w),y)\cdot p(y|x)dy</script><p>但是在测试集中需要最小化的是：</p><script type="math/tex; mode=display">minimize_w\int q(x)dx\int l(f(x,w),y)\cdot p(y|x)dy</script><p>为了进行修正，根据下面的公式，我们需要找到一个概率密度比率函数$\alpha(x)=\frac{q(x)}{p(x)}$</p><script type="math/tex; mode=display">\begin{align}\int q(x)f(x)dx&=\int p(x)\frac{q(x)}{p(x)}f(x)dx\\&=\int p(x)\alpha(x)f(x)dx\end{align}</script><p>但是我们并不知道$\alpha(x)$</p></li><li><p>解决方法：利用训练集和测试集训练出一个分类器</p><ul><li><p>当$x$来自训练集时$z=1$，否则$z=-1$，$r(z=1|x)=\frac{p(x)}{p(x)+q(x)}$，所以$\alpha(x)=\frac{r(z=1|x)}{r(z=-1|x)}$</p></li><li><p>利用logistic regression，$r(z=1|x)=\frac{1}{1+exp(-f(x))}$，$\alpha(x)=exp(f(x))$</p></li><li><p>之后通过训练二分类器可得$f(x)$</p></li><li><p>修正后的损失函数：</p><script type="math/tex; mode=display">minimize_w\frac1m\sum_{i=1}^ml(f(x_i,w), y_i)\to minimize_w\frac1m\sum_{i=1}^ml(f(x_i,w), y_i)\cdot exp(f(x_i))</script></li></ul></li></ul><h4 id="Label-Shift-Correction"><a href="#Label-Shift-Correction" class="headerlink" title="Label Shift Correction"></a>Label Shift Correction</h4><ul><li>仅做了解，可以参考<a href="http://d2l.ai/chapter_multilayer-perceptrons/environment.html" target="_blank" rel="noopener">d2l</a></li></ul><h4 id="Adversarial-data"><a href="#Adversarial-data" class="headerlink" title="Adversarial data"></a><strong>Adversarial data</strong></h4><ul><li><a href="https://courses.d2l.ai/berkeley-stat-157/slides/2_19/9-Environment.pdf" target="_blank" rel="noopener">slides</a></li></ul><h4 id="Nonstationary-Environments"><a href="#Nonstationary-Environments" class="headerlink" title="Nonstationary Environments"></a><strong>Nonstationary Environments</strong></h4><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/08/11/STAT157-L8-L9/环境.png" alt="环境" title="">                </div>                <div class="image-caption">环境</div>            </figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;STAT157-L8-L9笔记&quot;&gt;&lt;a href=&quot;#STAT157-L8-L9笔记&quot; class=&quot;headerlink&quot; title=&quot;STAT157 L8-L9笔记&quot;&gt;&lt;/a&gt;STAT157 L8-L9笔记&lt;/h1&gt;&lt;p&gt;关于课程&lt;a href=&quot;https://courses.d2l.ai/berkeley-stat-157/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;STAT157&lt;/a&gt;的学习笔记&lt;/p&gt;
&lt;p&gt;学习时间2019/8/10&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine lerning" scheme="http://yoursite.com/tags/machine-lerning/"/>
    
      <category term="STAT157" scheme="http://yoursite.com/tags/STAT157/"/>
    
  </entry>
  
  <entry>
    <title>STAT157 L6-L7</title>
    <link href="http://yoursite.com/2019/08/08/STAT157-L6-L7/"/>
    <id>http://yoursite.com/2019/08/08/STAT157-L6-L7/</id>
    <published>2019-08-08T00:34:00.000Z</published>
    <updated>2019-08-11T03:00:17.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="STAT157-L6-L7笔记"><a href="#STAT157-L6-L7笔记" class="headerlink" title="STAT157 L6-L7笔记"></a>STAT157 L6-L7笔记</h1><p>关于课程<a href="https://courses.d2l.ai/berkeley-stat-157/index.html" target="_blank" rel="noopener">STAT157</a>的学习笔记</p><p>学习时间2019/8/08</p> <a id="more"></a><h2 id="Multilayer-Perceptron"><a href="#Multilayer-Perceptron" class="headerlink" title="Multilayer Perceptron"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/mlp.html" target="_blank" rel="noopener">Multilayer Perceptron</a></h2><h3 id="Single-Layer-Perceptron"><a href="#Single-Layer-Perceptron" class="headerlink" title="Single Layer Perceptron"></a><strong>Single Layer Perceptron</strong></h3><ul><li><a href="https://en.wikipedia.org/wiki/Perceptron" target="_blank" rel="noopener">单层感知机</a></li></ul><p>需要注意的是单层感知机的训练算法，以及其算法的收敛性，这些在上面的链接中均有介绍</p><h3 id="Multilayer-Perceptron-1"><a href="#Multilayer-Perceptron-1" class="headerlink" title="Multilayer Perceptron"></a>Multilayer Perceptron</h3><ul><li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" target="_blank" rel="noopener">multilayer perceptron</a></li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/08/08/STAT157-L6-L7/multilayer" alt="multilayer perceptron" title="perceptron.png">                </div>                <div class="image-caption">perceptron.png</div>            </figure><ul><li>上面的定义来自<a href="https://zh.d2l.ai/d2l-zh.pdf" target="_blank" rel="noopener">中文版动手学深度学习</a>，若不做说明，下面的图都来自<a href="https://zh.d2l.ai/d2l-zh.pdf" target="_blank" rel="noopener">中文版动手学深度学习</a></li></ul><h2 id="Model-Selection-Weight-Decay-Dropout"><a href="#Model-Selection-Weight-Decay-Dropout" class="headerlink" title="Model Selection, Weight Decay, Dropout"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/capacity.html#model-selection-weight-decay-dropout" target="_blank" rel="noopener">Model Selection, Weight Decay, Dropout</a></h2><h3 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h3><ul><li><p>测试集&amp;验证集</p><blockquote><p>在机器学习中，通常需要评估若干候选模型的表现并从中选择模型。这一过程称为模型选择 (model selection)。可供选择的候选模型可以是有着不同超参数的同类模型。以多层感知机为 例，我们可以选择隐藏层的个数，以及每个隐藏层中隐藏单元个数和激活函数。为了得到有效的 模型，我们通常要在模型选择上下一番功夫。下面，我们来描述模型选择中经常使用的验证数据 集(validation data set)。 </p></blockquote></li><li><p>模型复杂度估计</p><ul><li>参数个数</li><li>参数取值范围</li><li>VC Dimension</li></ul></li><li><p>数据复杂度估计</p><ul><li>数据规模</li><li>数据特征数</li><li>数据结构</li><li>数据的多样性</li></ul></li></ul><h3 id="Weight-decay"><a href="#Weight-decay" class="headerlink" title="Weight decay"></a>Weight decay</h3><ul><li><blockquote><p>权重衰减等价于L2 范数正则化(regularization)。正则化通过为模型损失函数添加惩罚项使学出<br>的模型参数值较</p></blockquote><p><img src="/2019/08/08/STAT157-L6-L7/L2正则化.png" alt="L2正则化"></p></li></ul><h3 id="Drop-out"><a href="#Drop-out" class="headerlink" title="Drop out"></a>Drop out</h3><ul><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/08/08/STAT157-L6-L7/dropout.png" alt="dropout" title="">                </div>                <div class="image-caption">dropout</div>            </figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;STAT157-L6-L7笔记&quot;&gt;&lt;a href=&quot;#STAT157-L6-L7笔记&quot; class=&quot;headerlink&quot; title=&quot;STAT157 L6-L7笔记&quot;&gt;&lt;/a&gt;STAT157 L6-L7笔记&lt;/h1&gt;&lt;p&gt;关于课程&lt;a href=&quot;https://courses.d2l.ai/berkeley-stat-157/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;STAT157&lt;/a&gt;的学习笔记&lt;/p&gt;
&lt;p&gt;学习时间2019/8/08&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine lerning" scheme="http://yoursite.com/tags/machine-lerning/"/>
    
      <category term="STAT157" scheme="http://yoursite.com/tags/STAT157/"/>
    
  </entry>
  
  <entry>
    <title>STAT157 L5</title>
    <link href="http://yoursite.com/2019/08/01/STAT157-L5/"/>
    <id>http://yoursite.com/2019/08/01/STAT157-L5/</id>
    <published>2019-08-01T02:02:26.000Z</published>
    <updated>2019-08-08T00:23:42.499Z</updated>
    
    <content type="html"><![CDATA[<h1 id="STAT157-L5笔记"><a href="#STAT157-L5笔记" class="headerlink" title="STAT157 L5笔记"></a>STAT157 L5笔记</h1><p>关于课程<a href="https://courses.d2l.ai/berkeley-stat-157/index.html" target="_blank" rel="noopener">STAT157</a>的学习笔记</p><p>学习时间2019/8/01</p> <a id="more"></a><h2 id="Likelihood-Loss-Functions-Logisitic-Regression-Information-Theory"><a href="#Likelihood-Loss-Functions-Logisitic-Regression-Information-Theory" class="headerlink" title="Likelihood, Loss Functions, Logisitic Regression, Information Theory"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/loss.html#" target="_blank" rel="noopener">Likelihood, Loss Functions, Logisitic Regression, Information Theory</a></h2><p>这一节的内容感觉学起来有些吃力，其中有不少内容是本科时候已经接触过多次的，比如极大似然，但是和之前的朴素贝叶斯<a href="http://alephn.xin/2019/07/31/STAT-157-L1-L2/" target="_blank" rel="noopener">STAT-157 L1-L2</a>一样，由于缺乏思考和锻炼，掌握的一直很不牢靠。由于课程中对于这些知识讲解的也比较简略（毕竟默认是先修课程的内容），因此在下面的笔记中会引用到陈希儒教授的<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwj8gPmbzeDjAhUZ7mEKHehSBdkQFjAAegQIARAB&amp;url=https%3A%2F%2Fbook.douban.com%2Fsubject%2F2201479%2F&amp;usg=AOvVaw0RKsZINYs95UOk8xqkATf4" target="_blank" rel="noopener">《概率论与数理统计》</a></p><h3 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a><strong>Maximum Likelihood</strong></h3><ul><li><p>什么是参数估计？</p><ul><li><blockquote><p>设有一个统计总体，以$f(x; \theta_1,\cdots,\theta_k)$记做其总体分布，具体含义视其是连续型还是离散型而定。这个分布包含了$k$个未知参数$\theta_1,\cdots,\theta_k$，参数估计问题的一般提法是：设有了从总体中抽取出的独立同分布样本$X_1, X_2, \cdots, X_n$，要依据这些样本去对$\theta_1,\cdots,\theta_k$的未知值进行估计</p></blockquote></li></ul></li><li><p>什么是极大似然估计？</p><ul><li><p>极大似然估计，即<strong>Maximum likelihood estimation(MLE)</strong></p></li><li><blockquote><p>设总体分布为$f(x; \theta_1,\cdots,\theta_k)$，那么样本$X_1, X_2, \cdots, X_n$的分布（概率密度函数或者概率函数）为</p><p>$f(X_1; \theta_1,\cdots,\theta_k)f(X_2; \theta_1,\cdots,\theta_k)\cdots f(X_n; \theta_1,\cdots,\theta_k)$</p><p>记做: $L(X_1, X_2,\cdots, L_n;  \theta_1,\cdots,\theta_k)$</p><p>固定$\theta_1,\cdots,\theta_k$而看作$X_1, X_2, \cdots, X_n$的函数，$L$是一个概率密度函数或概率函数，当把$X_1, X_2, \cdots, X_n$固定而把$\theta_1,\cdots,\theta_k$时称$L$为似然函数，此时$L$反映了在观察结果$X_1, X_2, \cdots, X_n$已知条件下，$\theta_1,\cdots,\theta_k$的各种值的”似然程度”，当满足条件：</p><p>$L(X_1, X_2,\cdots, L_n;  \theta_1^<em>,\cdots,\theta_k^</em>)=max_{\theta_1,\cdots,\theta_k}L(X_1, X_2,\cdots, L_n;  \theta_1,\cdots,\theta_k)$</p><p>我们就用$\theta_1^<em>,\cdots,\theta_k^</em>$作为未知参数的估计</p></blockquote></li><li><p>一般而言，我们会通过求解$logL=\sum_{i=1}^{n}logf(X_i;\theta_1,\cdots,\theta_k)$的最大值得到$\theta_1^<em>,\cdots,\theta_k^</em>$</p></li></ul></li><li><p>什么是最大后验估计？</p><ul><li><p>最大后验估计，即<strong>Maximum a posteriori estimation(MAP)</strong></p></li><li><p>在极大似然估计中，我们最优化的目标是:$f(X_1; \theta_1,\cdots,\theta_k)f(X_2; \theta_1,\cdots,\theta_k)\cdots f(X_n; \theta_1,\cdots,\theta_k)$，也可以理解为$P(X|\theta)$，我们认为$\theta$是一个确定的未知值，但是在最大后验估计中，我们优化的是$P(\theta|X)$，也就是需要最优化$P(X|\theta)\cdot P(\theta)$</p></li><li><script type="math/tex; mode=display">P(\theta|X)=\frac{P(X|\theta)\cdot P(\theta)}{P(X)}</script></li></ul></li></ul><h3 id="Maximum-Likelihood-for-Simple-Linear-Regression"><a href="#Maximum-Likelihood-for-Simple-Linear-Regression" class="headerlink" title="Maximum Likelihood for Simple Linear Regression"></a>Maximum Likelihood for Simple Linear Regression</h3><p>这一小节让我对线性回归有了一个新的认识！虽然一些细节还是不太理解，但是这里还是勉强进行一个总结</p><ul><li>首先回顾一下线性回归问题：给定一系列数据$ (y_i,x_{i1},…,x_{ip})_{i=1}^n$，构建一个线性模型$\hat y=WX+b$来对所有数据进行拟合。一般来说，我们会利用L2 loss来对这个问题进行求解，但是一直以来，好像都没有去思考为什么优化的的L2 loss？</li><li><p><img src="/2019/08/01/STAT157-L5/definition-1.png"></p></li><li><p>利用上面的定义，我们就能把线性回归和概率模型结合起来了（实际上维基百科就是这么定义的，只是这么多年学习线性回归时一直没有接触过。。。），我们最终优化的目标，其实是根据MAP（MLE）来确定的</p><p><img src="/2019/08/01/STAT157-L5/true.png" alt="true"></p></li><li><p>上面就是在给定模型下，$(x, y)$出现的概率密度函数，由于参数未知，将模型写为：</p><p><img src="/2019/08/01/STAT157-L5/predict.png" alt="predict"></p></li><li><p>似然函数L为：</p><p><img src="/2019/08/01/STAT157-L5/L.png" alt="L"></p></li><li><p>最优解为：</p><p><img src="/2019/08/01/STAT157-L5/solution.png" alt="solution"></p></li><li><p>当我们假设误差$\epsilon ～N(0, \sigma^2)$时，参数与最优化L2 loss的解相同。</p></li><li><p>课程中将上面似然函数的优化直接等价于(如下图)最优化L2 loss，这里我不是非常理解</p><p><img src="/2019/08/01/STAT157-L5/course.png" alt="course"></p></li></ul><p>上面的公式均出自<a href="http://www.stat.cmu.edu/~cshalizi/mreg/15/" target="_blank" rel="noopener">36-401, Modern Regression, Section B</a></p><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>很抱歉，这一节我基本没有看懂老师在说些什么。。。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/08/01/STAT157-L5/loss.png" alt="loss" title="">                </div>                <div class="image-caption">loss</div>            </figure><p>这张图里面的箭头含有究竟是指什么？或许这部分并不是很重要，暂且略过</p><h3 id="logistic-regression"><a href="#logistic-regression" class="headerlink" title="logistic regression"></a>logistic regression</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/08/01/STAT157-L5/crossEntropy.png" alt="crossEntropy" title="">                </div>                <div class="image-caption">crossEntropy</div>            </figure><h3 id="information-theory"><a href="#information-theory" class="headerlink" title="information theory"></a>information theory</h3><ul><li><p>Entropy：$H[p]=-\sum_i(p_i\cdot lnp_i)$，根据维基百科</p><blockquote><p>在<a href="https://zh.wikipedia.org/wiki/信息论" target="_blank" rel="noopener">信息论</a>中，<strong>熵</strong>（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为<strong>信息熵</strong>、<strong>信源熵</strong>、<strong>平均自信息量</strong>。</p></blockquote></li><li><p><strong>Kullback-Leibler Divergence</strong>(KL散度)</p><ul><li><blockquote><p><strong>KL散度</strong>是两个概率分布P和Q差别的非对称性的度量。 KL散度是用来 度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。</p><p><a href="https://zh.wikipedia.org/wiki/相对熵" target="_blank" rel="noopener">维基百科</a></p></blockquote></li><li><p>对于离散型随机变量，KL散度为：$D(P||Q)=-\sum_i(P_i(\frac{lnP_i}{lnQ_i}))$</p></li><li><script type="math/tex; mode=display">\begin{align}D(y||softmax(o))&=-\sum_iy_ilny_i+\sum_iy_ilnsoftmax(o)_i\\&=H[y]+(y^To-\sum_iexp(o_i))\end{align}</script></li></ul></li></ul><h3 id="课程笔记"><a href="#课程笔记" class="headerlink" title="课程笔记"></a>课程笔记</h3><ul><li><a href="https://github.com/waxin/STAT-157/blob/master/Notebooks/L5/logistic_regression.ipynb" target="_blank" rel="noopener">logistic regression</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;STAT157-L5笔记&quot;&gt;&lt;a href=&quot;#STAT157-L5笔记&quot; class=&quot;headerlink&quot; title=&quot;STAT157 L5笔记&quot;&gt;&lt;/a&gt;STAT157 L5笔记&lt;/h1&gt;&lt;p&gt;关于课程&lt;a href=&quot;https://courses.d2l.ai/berkeley-stat-157/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;STAT157&lt;/a&gt;的学习笔记&lt;/p&gt;
&lt;p&gt;学习时间2019/8/01&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine lerning" scheme="http://yoursite.com/tags/machine-lerning/"/>
    
      <category term="STAT157" scheme="http://yoursite.com/tags/STAT157/"/>
    
  </entry>
  
  <entry>
    <title>STAT-157 L1-L2</title>
    <link href="http://yoursite.com/2019/07/31/STAT-157-L1-L2/"/>
    <id>http://yoursite.com/2019/07/31/STAT-157-L1-L2/</id>
    <published>2019-07-31T13:25:46.000Z</published>
    <updated>2019-07-31T13:45:27.138Z</updated>
    
    <content type="html"><![CDATA[<h1 id="STAT157-L1-L2笔记"><a href="#STAT157-L1-L2笔记" class="headerlink" title="STAT157 L1-L2笔记"></a>STAT157 L1-L2笔记</h1><p>关于课程<a href="https://courses.d2l.ai/berkeley-stat-157/index.html" target="_blank" rel="noopener">STAT157</a>的学习笔记</p><p>学习时间2019/7/29 </p><a id="more"></a><h2 id="Logistics-Software-Linear-Algebra"><a href="#Logistics-Software-Linear-Algebra" class="headerlink" title="Logistics, Software, Linear Algebra"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/introduction.html" target="_blank" rel="noopener">Logistics, Software, Linear Algebra</a></h2><ul><li>主要介绍相关软件以及线性代数的基础知识，由于我使用的设备不支持GPU，所以不考虑使用推荐的软件，关于线性代数的作业也比较基础，并且之前在其它课程中对向量、矩阵等运算已经有了不少训练，因此这一部分暂且略过</li></ul><h2 id="Probability-and-Statistics-Bayes-Rule-Sampling-Naive-Bayes-Sampling"><a href="#Probability-and-Statistics-Bayes-Rule-Sampling-Naive-Bayes-Sampling" class="headerlink" title="Probability and Statistics (Bayes Rule, Sampling Naive Bayes, Sampling)"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/probability.html" target="_blank" rel="noopener">Probability and Statistics (Bayes Rule, Sampling Naive Bayes, Sampling)</a></h2><ul><li><p>朴素贝叶斯：课程中以垃圾邮件分类为例大致讲解了朴素贝叶斯的原理，但是非常简略，以至于我没有听懂。。。虽然之前已经学过朴素贝叶斯，但是很久没有接触导致忘的差不多了，下面是根据一篇<a href="https://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html" target="_blank" rel="noopener">教程</a>对朴素贝叶斯的一个回顾</p><ul><li><p>假设待分类项$x$具有特征$a_1, a_2, \cdots, a_n$，（例如对于邮件而言可以把单词是否出现作为特征），$x$可能的分类有$y_1, y_2,\cdots, y_m$，我们需要对给定的$x$确定最优分类$y_{max}$</p></li><li><p>如何利用朴素贝叶斯的原理来解决上面的问题呢？</p><ul><li><p>首先看一下条件概率的计算</p><script type="math/tex; mode=display">\begin{align}P(A|B)&=\frac{P(AB)}{P(B)}\\&=\frac{P(B|A)P(A)}{P(B)}\end{align}</script><p>将$B$看作是特征，$A$看作分类，我们要求的是$P(y=y_i|x=(a_1, a_2, \cdots, a_n))$，因为对于同一个分类项$x$来说，$P(x=(a_1, a_2, \cdots, a_n))$是一个常数，因此在计算不同可能分类的条件概率时我们不用考虑它，也就是说$P(y=y_i|x=(a_1, a_2, \cdots, a_n))\propto P(x=(a_1, a_2, \cdots, a_n)|y=y_i)\cdot P(y_i)$</p><p>当认为各个特征选择完全独立时，$P(x=(a_1, a_2, \cdots, a_n)|y=y_i)=P(x_1=a_1|y=y_i)\cdots  P(x_n=a_n|y=y_i)$</p><p>即：$P(y=y_i|x=(a_1, a_2, \cdots, a_n))\propto P(x_1=a_1|y=y_i)\cdots  P(x_n=a_n|y=y_i)\cdot P(y_i)$，也是课程中的说明。</p></li><li><p>有了上面的简要介绍，接下来我们可以针对具体问题来利用朴素贝叶斯来构建分类模型了。与课程一样，这里以MNIST手写数字为例，在MNIST数据集中，$x$可以看作一张图片的所有像素点，每个像素点的取值选择为0/1，$y$总共有十种，从0-9。由此，我们基本对问题定义有了大概的了解，接着需要利用训练集计算$P(x_i=1|y_j)$以及$P(y=y_j)$其中$ i=1～784, j=0～9$，$i$表示像素点的序号，对于测试样例，只要计算$P(x_1=a_1|y=y_j)\cdots  P(x_{784}=a_n|y=y_j)\cdot P(y_j)$取得最大值的$j$即为预测结果。</p></li><li><p>具体的分类器构建可以参考<a href="https://courses.d2l.ai/berkeley-stat-157/slides/1_24/naive-bayes.ipynb" target="_blank" rel="noopener">STAT 156 Naive Bayes</a>，为了加深对朴素贝叶斯的理解，我自己也构建了一个分类器，<a href="https://github.com/waxin/STAT-157/blob/master/Notebooks/L2/NaiveBayesClassifier.ipynb" target="_blank" rel="noopener">Naive Bayes Classifier</a></p><ul><li><p>在自己实现分类器的过程中，我认为有两点需要注意，其一是$P(x_i=1|y_j)$计算时分母是$y_i$对应的统计数目而非整个训练集大小，其二可以认为是一个小track，在某些情况下$P(x_i=1|y_j)$可能为0，因为需要进行$log$操作，所以这时会报<code>warning</code>，为了避免，可以按照下面的方式初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xcount = np.ones((<span class="number">10</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">ycount = np.ones((<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>大数定理&amp;中心极限定理</p><p>课程中其它部分主要涉及了概率论的基础知识，介绍了一些概率分布（笔记本中有涉及到），我认为能够掌握大数定理和中心极限定理，对理解课程中的内容有很大帮助，下面简要介绍</p><ul><li><p>大数定理：</p><p>根据<a href="https://baike.baidu.com/item/大数定律" target="_blank" rel="noopener">百度百科</a></p><blockquote><p>伯努利大数定律：设$\mu$是n次独立试验中事件A发生的次数，且事件A在每次试验中发生的概率为$p$，则对任意正数ε:</p><p>$\lim_{n\to\infty}P(|\frac{\mu_n}{n}-p|&lt;\epsilon)=1$</p><p>其含义是，当n足够大时，事件A出现的频率将几乎接近于其发生的概率，即频率的稳定性。</p></blockquote><p>这解释了课程<a href="https://courses.d2l.ai/berkeley-stat-157/slides/1_24/probability.ipynb" target="_blank" rel="noopener">notebook</a>中模拟多次掷骰子的实验结果</p></li><li><p>中心极限定理：根据《概率论与数理统计》陈希儒版</p><blockquote><p>在概率论中，习惯于把和的分布收敛与正态分布的那一类定理叫做“中心极限定理”，下面的定理就是其中之一：</p><script type="math/tex; mode=display">\begin{align}&设X_1, X_2,\cdots X_n为独立同分布的随机变量，E(X_i)=a, Var(X_i)=\sigma^2,0<\sigma^2<\infty,\\&则对任意实数x有：\\&\lim_{n\to\infty}P(\frac1{\sqrt{n}\sigma}(X_1+X_2+\cdots+X_n-na)\leq x)=\Phi(x)\\&\Phi(x)是服从N(0, 1)的正态分布的分布函数\end{align}</script><p>$\frac1{\sqrt{n}\sigma}(X_1+X_2+\cdots+X_n-na)$表示$X_1+X_2+\cdots+X_n$的标准化，即均值为0，方差为1</p></blockquote><p>课程中有可视化中心极限定理，这里我自己根据上面的定义，也做了一个简单的<a href="https://github.com/waxin/STAT-157/blob/master/Notebooks/L2/CentralLimitTheorem.ipynb" target="_blank" rel="noopener">示例</a></p></li></ul></li></ul><p><font color="#3333ff">通过简单的代码，我对今天课程中涉及的一些原理有了大致了解，但是要真正的理解还需要之后在实践中多加思考消化。其实朴素贝叶斯，大数定理以及中心极限定理我都学过，但是再次遇见时仍然需要再次学习，一来与之前学习的不够透彻有关，二来是因为平时很少接触，总之，纸上得来终觉浅，绝知此事要躬行。</font></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;STAT157-L1-L2笔记&quot;&gt;&lt;a href=&quot;#STAT157-L1-L2笔记&quot; class=&quot;headerlink&quot; title=&quot;STAT157 L1-L2笔记&quot;&gt;&lt;/a&gt;STAT157 L1-L2笔记&lt;/h1&gt;&lt;p&gt;关于课程&lt;a href=&quot;https://courses.d2l.ai/berkeley-stat-157/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;STAT157&lt;/a&gt;的学习笔记&lt;/p&gt;
&lt;p&gt;学习时间2019/7/29 &lt;/p&gt;
    
    </summary>
    
    
      <category term="machine lerning,STAT157" scheme="http://yoursite.com/tags/machine-lerning-STAT157/"/>
    
  </entry>
  
  <entry>
    <title>STAT157 L3-L4</title>
    <link href="http://yoursite.com/2019/07/31/STAT157-L3-L4/"/>
    <id>http://yoursite.com/2019/07/31/STAT157-L3-L4/</id>
    <published>2019-07-31T00:57:19.000Z</published>
    <updated>2019-07-31T13:27:46.283Z</updated>
    
    <content type="html"><![CDATA[<h1 id="STAT157-L3-L4笔记"><a href="#STAT157-L3-L4笔记" class="headerlink" title="STAT157 L3-L4笔记"></a>STAT157 L3-L4笔记</h1><p>关于课程<a href="https://courses.d2l.ai/berkeley-stat-157/index.html" target="_blank" rel="noopener">STAT157</a>的学习笔记</p><p>学习时间2019/7/30 - 2019/7/31</p> <a id="more"></a><h2 id="Gradients-Chain-Rule-Automatic-Differentiation"><a href="#Gradients-Chain-Rule-Automatic-Differentiation" class="headerlink" title="Gradients, Chain Rule, Automatic Differentiation"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/arrays.html#gradients-chain-rule-automatic-differentiation" target="_blank" rel="noopener">Gradients, Chain Rule, Automatic Differentiation</a></h2><p>这一节主要讲述梯度的运算技巧以及自动微分</p><h3 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h3><ul><li><p>梯度运算</p><ul><li><p>课程中关于各种梯度的运算可以总结为下面的<a href="https://courses.d2l.ai/berkeley-stat-157/slides/1_29/3-Gradient.pdf" target="_blank" rel="noopener">一张图</a></p><p align="center"><img src="/2019/07/31/STAT157-L3-L4/stat157-gradient.png" height="250"></p><p align="center"><font color="#333fff" size="2.8">图1:梯度运算总结</font></p><p>按照课程，常数对纵向量的梯度是横向量，这里让我比较疑惑，之前关于梯度的运算我费了很大功夫，虽然没有完全理解，但是基本能计算一些神经网络的反向传播过程，这里我觉得STAT157关于梯度运算讲解的不是很详细，可以参考<a href="http://cs231n.stanford.edu/handouts/derivatives.pdf" target="_blank" rel="noopener">Derivatives, Backpropagation, and Vectorization</a>以及<a href="http://cs231n.stanford.edu/vecDerivs.pdf" target="_blank" rel="noopener">Vector, Matrix, and Tensor Derivatives</a></p></li></ul></li><li><p>自动微分</p><p>自动微分区别于符号微分与数值微分，例如在神经网络的反向传播过程中，我们需要计算损失函数对于各个参数的梯度，然后根据公式来实现代码，这个过程需要一定的数学基础（我现在也只是大概能求解，Jacobian矩阵真的不是很好理解）。在很多深度学习框架中，例如tensorflow，以及本课程用到的MXnet都使用了自动微分技术，以达到向用户隐藏微分的细节。为了更好的说明自动微分，下面没有使用STAT课件中的例子，而是采纳了<a href="https://blog.csdn.net/aws3217150/article/details/70214422" target="_blank" rel="noopener">自动微分(Automatic Differentiation)简介</a></p><ul><li><p align="center"><img src="/2019/07/31/STAT157-L3-L4/autodiff.png" height="200"></p><p align="center"><font color="#333fff" size="2.8">图2:Autodiff计算图</font></p></li><li><p>前向自动微分：图2表示的是一个2输入，1输出的计算图，在前向自动微分中，我们计算<strong>每个节点对输入</strong>的微分，整个过程从输入到输出，例如$v_0=f_1(x2), v_2=f_2(v1)$，$v_0^{‘}=f_1^{‘}, v_2^{‘}=f_2^{‘}\cdot v_0^{‘}$</p></li><li><p>反向自动微分：与前向自动微分不同，反向自动微分计算的是<strong>输出对每个节点</strong>的微分，这个过程从输出到输入，与反向传播过程类似。</p></li></ul><p><a href="https://blog.csdn.net/aws3217150/article/details/70214422" target="_blank" rel="noopener">自动微分(Automatic Differentiation)简介</a>中有更加详细的介绍</p></li></ul><h3 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h3><ul><li><p>在运行课程<a href="https://courses.d2l.ai/berkeley-stat-157/slides/1_29/autograd.ipynb" target="_blank" rel="noopener">notebook</a>时，下面的现象让我有些疑惑</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = mx.nd.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">x.attach_grad()</span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    y = x * <span class="number">2</span></span><br><span class="line">    <span class="comment">#y.attach_grad()</span></span><br><span class="line">    z = y * x</span><br></pre></td></tr></table></figure></li><li><p>在运行上面的代码时，对于y是否进行<code>attach_grad</code>操作，得到的结果会不同，如果不进行<code>attach_grad</code>操作，<code>x.grad</code></p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">4.</span>  <span class="number">8.</span>]</span><br><span class="line"> [<span class="number">12.</span> <span class="number">16.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x2 @cpu(<span class="number">0</span>)&gt;</span><br></pre></td></tr></table></figure><p>否则为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">2.</span> <span class="number">4.</span>]</span><br><span class="line"> [<span class="number">6.</span> <span class="number">8.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x2 @cpu(<span class="number">0</span>)&gt;</span><br></pre></td></tr></table></figure><p>猜测是，对y进行<code>attach_grad</code>操作后，计算过程为$\frac {\partial z}{\partial x}=\frac {\partial z}{\partial y}\cdot\frac {\partial y}{\partial x}$，但是程序计算$\frac {\partial z}{\partial y}=x$，这其实是不正确的，利用符号微分：</p><script type="math/tex; mode=display">\begin{align}\frac {\partial z}{\partial y} &= \frac {\partial y}{\partial y}\cdot x+\frac {\partial x}{\partial y}\cdot y\\&=x+\frac 1 2\cdot2 x\\&=2x\end{align}</script><p>这里给我造成了一些疑惑，课程中并没有进行解释，暂且搁置</p></li></ul><p>关于MXnet Autograd的更多内容，可以参考<a href="https://mxnet.incubator.apache.org/api/python/autograd/autograd.html" target="_blank" rel="noopener">Autograd Package</a></p><h2 id="Linear-Regression-Basic-Optimization"><a href="#Linear-Regression-Basic-Optimization" class="headerlink" title="Linear Regression, Basic Optimization"></a><a href="https://courses.d2l.ai/berkeley-stat-157/units/linear.html#" target="_blank" rel="noopener">Linear Regression, Basic Optimization</a></h2><p>这一小节主要介绍了线性回归模型，并且讲述了如何利用MXnet来进行求解</p><h3 id="课程内容-1"><a href="#课程内容-1" class="headerlink" title="课程内容"></a>课程内容</h3><ul><li>通过预测房价的例子介绍了线性回归模型的定义</li><li>介绍了mini-batch SGD</li></ul><p>因为线性回归模型比较简单，并且我基本还记得，这里对于课程内容就不做过多记录</p><h3 id="Jupyter-Notebook-1"><a href="#Jupyter-Notebook-1" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h3><ul><li>课程中介绍了两种方式实现mini-batch SGD，一种是自己一步一步构建模型，包括前向反馈以及反向传播过程，这与吴恩达老师的<a href="https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning" target="_blank" rel="noopener">神经网络与深度学习</a>中的练习基本类似，不过这里可以利用mxnet的自动微分来进行反向传播过程中的梯度运算，另一种是利用mxnet gluon来搭建模型</li><li>在<a href="https://github.com/waxin/STAT-157/blob/master/Notebooks/L4/Linear%20Regression.ipynb" target="_blank" rel="noopener">Linear regression</a>中利用上述两种方式完成了模型的构建及优化（数据部分采用课程<a href="https://courses.d2l.ai/berkeley-stat-157/slides/1_31/linear-regression-scratch.ipynb" target="_blank" rel="noopener">Notebook</a>）</li></ul><p>关于mxnet gluon可以参考<a href="https://beta.mxnet.io/guide/crash-course/2-nn.html" target="_blank" rel="noopener">Create a neural network</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;STAT157-L3-L4笔记&quot;&gt;&lt;a href=&quot;#STAT157-L3-L4笔记&quot; class=&quot;headerlink&quot; title=&quot;STAT157 L3-L4笔记&quot;&gt;&lt;/a&gt;STAT157 L3-L4笔记&lt;/h1&gt;&lt;p&gt;关于课程&lt;a href=&quot;https://courses.d2l.ai/berkeley-stat-157/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;STAT157&lt;/a&gt;的学习笔记&lt;/p&gt;
&lt;p&gt;学习时间2019/7/30 - 2019/7/31&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine lerning" scheme="http://yoursite.com/tags/machine-lerning/"/>
    
      <category term="STAT157" scheme="http://yoursite.com/tags/STAT157/"/>
    
  </entry>
  
  <entry>
    <title>总结—2019/7/27</title>
    <link href="http://yoursite.com/2019/07/28/%E6%80%BB%E7%BB%93%E2%80%942019-7-26/"/>
    <id>http://yoursite.com/2019/07/28/总结—2019-7-26/</id>
    <published>2019-07-28T09:42:06.000Z</published>
    <updated>2019-07-28T10:03:43.086Z</updated>
    
    <content type="html"><![CDATA[<h1 id="总结-2019-7-27"><a href="#总结-2019-7-27" class="headerlink" title="总结-2019/7/27"></a>总结-2019/7/27</h1><p>下面主要对<strong>7-22～7-26</strong>这几天学习的一些知识进行一次回顾</p><a id="more"></a><h2 id="主要学习资料"><a href="#主要学习资料" class="headerlink" title="主要学习资料"></a>主要学习资料</h2><ul><li><a href="https://www.coursera.org/learn/deep-neural-network" target="_blank" rel="noopener">吴恩达coursera课程</a></li><li><a href="http://cs231n.stanford.edu/handouts/derivatives.pdf" target="_blank" rel="noopener">Derivatives, Backpropagation, and Vectorization</a></li><li><a href="https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528" target="_blank" rel="noopener">Xavier and He Normal (He-et-al) Initialization</a></li></ul><h2 id="内容回顾"><a href="#内容回顾" class="headerlink" title="内容回顾"></a>内容回顾</h2><h3 id="part1：手工构建神经网络"><a href="#part1：手工构建神经网络" class="headerlink" title="part1：手工构建神经网络"></a>part1：手工构建神经网络</h3><p>​    在这一周，通过课程我学习了如何利用numpy等python库来搭建一个多层的神经网络，并且对其进行训练。多层神经网络的结构如下图所示：</p><p align="center"><img src="/2019/07/28/总结—2019-7-26/network.png" width="300" height="200"></p><p align="center"><font size="2.8" color="#3333ff">图1:多层神经网络结构</font></p><p>​        为了构建这样一个多层的神经网络，大致需要经过以下几个步骤</p><ul><li><p>参数的初始化：例如从输入到隐藏层，我们需要的是权值矩阵以及偏置，这在一开始就需要进行初始化。一般来说，初始化所有的参数，需要先定义出各层的规模。参数的初始化对之后的训练会起到一定影响，具体会在之后说明。</p></li><li><p>前向反馈过程：有了参数以及输入，我们需要实现上述神经网络的计算过程。以图一为例（参考其网络结构，但是更改输入为纵向量），整个网络的计算过程如下：</p><script type="math/tex; mode=display">Z_1 = W_1*X+b_1,\ \ \ \ \ A_1 = f_1(Z_1)\\Z_2 = W_2*A_1+b_2, \ \ \ \ A_2 = f_2(Z_2)\\Z_3 = W_o*A_2+b_3,\ \ \ \ \ \ \hat Y = f_3(Z_3)</script><p>其中$f$表示激活函数，常见的有$relu,  sigmoid$等，最后为了得到输出，一般会使用$sigmoid$或$softmax$，具体需要看是二分类还是多分类问题。</p></li><li><p>计算损失函数：上述神经网络最后可以计算得到预测结果$\hat y$，以二分类问题为例，对数似然损失函数如下：</p><script type="math/tex; mode=display">loss = -[y*log(\hat y)\ +\ (1-y)*log(1-\hat y)]</script><p>上面展示的是对一条数据的计算过程，当进行向量化处理时需要求均值。</p></li><li><p>反向传播过程：利用反向传播，我们可以计算出损失函数对各个参数的导数，之后对各参数进行更新以降低模型在训练数据上的损失，假设训练数据的规模为$m$，并且输出层为$sigmoid$，那么图1的反向传播可以表示如下：</p><script type="math/tex; mode=display">loss = -1/m*[Y*log(\hat Y)\ +\ (1-Y)*log(1-\hat Y)]\\dZ_3 = \hat Y\ -\ Y\\\ \\dW_o = 1/m*dZ_3*A_2.T,\ db_3 = 1/m*np.sum(dZ_3, axis=1)\\dA_2 = W_o.T*dZ_3,\ dZ_2 = dA_2*f_2^{'}(Z_2)\\\ \\dW_2 = 1/m*dZ_2*A_1.T,\ db_2 = 1/m*np.sum(dZ_2, axis=1)\\dA_1 = W_2.T*dZ_2,\ dZ_1 = dA_1*f_1^{'}(Z_1)\\\ \\dW_1 = 1/m*dZ_1*A_1.T,\ db_1 = 1/m*np.sum(dZ_1, axis=1)</script></li><li><p>模型的组合：利用上述的几个部分，可以定义出一个神经网络模型，在训练集上进行多次迭代，不断更新参数以降低损失函数，最后得到模型的参数</p></li><li><p>预测：利用学习得到的参数以及前向反馈过程就可以对测试集进行预测</p></li></ul><h3 id="part2：参数初始化"><a href="#part2：参数初始化" class="headerlink" title="part2：参数初始化"></a>part2：参数初始化</h3><p>在part1中已经提到模型的参数初始化会对模型训练产生影响，下面就对参数初始化进行具体介绍。</p><ul><li><p>Zero Initialization</p><ul><li><p>将所有的权值矩阵初始化为0，假设中间层的激活函数是$relu$，输出层的激活函数为$sigmoid$</p><script type="math/tex; mode=display">\begin{align}dW_1 &= dZ_1*X.T \\&=dA_1*f^{'}(Z_1)*X.T\\&=W_2.T*dZ_2*f^{'}(Z_1)*X.T\end{align}</script><p>可以看到在第一次迭代当中，$dW_1$受到$W_2$的影响结果为0，因此不会对$W_1$更新，类似的其它权值矩阵相同。对于$dW_L$:</p><script type="math/tex; mode=display">\begin{align}dW_L &=dZ_L*A_{L-1}.T\\&=(A_L-Y)*A_{L-1}.T\end{align}</script><p>因为中间层激活函数是$relu$，因此$dW_L$为0。如果激活函数是$tanh$，那么在第一轮迭代后会对$W_L$更新。但是由于权值矩阵中的每一行都是相同的（行向量可以看作是对输入的一个映射），根据对称性，在同一层的神经元作用相同，因此完全不推荐使用<strong>Zero Initialization</strong></p></li></ul></li><li><p>Random Initialization</p><ul><li>为了使每个神经元发挥不同的作用，我们需要保证权值矩阵中的每行向量具有区分度，因此我们可以利用Random Initialization来进行初始化。利用<code>np.random.randn</code>可以得到服从$N(\mu=0, \delta^2=1)$的特定$shape$的矩阵。但是初始化时权值矩阵的方差选择会成为比较棘手的问题，当选择过小时可能会出现梯度消失，导致训练速度过慢，当选择过大时可能会出现梯度爆炸，导致无法训练。</li></ul></li><li><p>HE Normal Initialization</p><ul><li>下面以中间层激活函数为$relu$的神经网络作为说明<script type="math/tex; mode=display">\begin{align}Z_1 &= W_1*X\\A_1 &= relu(Z_1)\\Z_{1_{ij}} &= \sum_{k=0}^{k=nc}W_{ik}\cdot X_{kj}\end{align}</script>$nc$表示$X$的特征数，或者$W_1$的列数。其中假设$W_1\sim N(0, 1), X\sim N(0, 1)$，利用基础的概率轮知识可以得到$Z_1\sim N(0, nc)$，也就是说经过$W_1$后$Z_1$的方差变大了，为了尽可能优化后面的反向传播过程，这里将$W_1$的方差设置为$2/nc$，分子为2是因为采用的是$relu$函数，这种初始化的方法叫做：HE Normal Initialization</li></ul></li></ul><hr><ul><li>[ ] Q2.1:关于这里的初始化我有一些不理解的地方，比如激活函数为$relu$时为什么分子选择2，$tanh$时选择分子为1？从$Z_1$到$A_1$，由于经过了一个非线性的映射，所以输出的分布完全变化了，在这时是否还是根据的方差或者均值来选择的分子呢？要解决这个疑问，后续可能需要阅读提出He-et-al的论文。</li></ul><hr><h3 id="part3：正则化"><a href="#part3：正则化" class="headerlink" title="part3：正则化"></a>part3：正则化</h3><p>在机器学习的过程中，我们最终需要的是模型在测试集上取得优异的表现，但是在有些时候，模型可能会发生过拟合的情况，例如下图：</p><p align="center"><img src="/2019/07/28/总结—2019-7-26/overfitting.png" height="200"></p><p align="center"><font color="#3333ff" size="2.8">图2：Over-fitting</font></p><ul><li><p>$L_2$正则化</p><ul><li>$loss = 1/m\cdot \zeta(\hat Y,Y)+\frac{\lambda}{2m}\cdot||\theta||^2$</li><li>$\frac{\lambda}{2m}\cdot||\theta||^2$表示正则化项，将损失函数加上$L_2$正则化项后可以避免权值矩阵过大，以此可以降低过拟合</li></ul></li><li><p>Dropout</p><ul><li>dropout，即随机失活，意味着在神经网络中随机对一些神经元进行失活操作，具体的操作需要一个失活矩阵$D$来实现：<script type="math/tex; mode=display">\begin{align}Z_i &= W_i*A_{i-1}\\A_i &= f(Z_i)\\A_i &= A_i\cdot D_i/keep_prob\end{align}</script>其中矩阵$D_i$是根据$keep_prob$来确定的0/1矩阵</li></ul></li></ul><hr><ul><li>[ ] Q3.1：$L_2$正则化有利于降低权值矩阵大小，但是权值矩阵的绝对大小对整个神经网络的效果会有什么样的影响呢？如何用数学化的形式具体说明$L_2$正则化是如何降低过拟合的呢？</li><li>[ ] Q3.2：在课程中，dropout是在激活层之后使用的，如果在激活层之前dropout会有什么区别呢？</li><li>[ ] Q3.3：dropout后保持输出的均值相同有什么作用 (利用$/keep_prob$保持均值)</li><li>[ ] Q3.4：dropout是如何降低过拟合的？</li></ul><hr><h3 id="part4：神经网络的优化"><a href="#part4：神经网络的优化" class="headerlink" title="part4：神经网络的优化"></a>part4：神经网络的优化</h3><p>Gradient descent，也就是梯度下降，是课程中一开始就使用的优化算法。根据维基百科:</p><blockquote><p>梯度下降方法基于以下的观察：如果实值函数$\displaystyle F(\mathbf {x} )$在点$\displaystyle \mathbf {a} $处<a href="https://zh.wikipedia.org/wiki/可微" target="_blank" rel="noopener">可微</a>且有定义，那么函数$\displaystyle F(\mathbf {x} )$在$\displaystyle \mathbf {a} $点沿着<a href="https://zh.wikipedia.org/wiki/梯度" target="_blank" rel="noopener">梯度</a>相反的方向 $\displaystyle -\nabla F(\mathbf {a} )$ 下降最快。</p></blockquote><p>因此，梯度下降通常也称为最速下降法。但是在数据规模很大时，梯度下降的成本非常高，因此需要进行一些转变</p><ul><li><p>Mini-batch graillent decent: 小批量梯度下降，假设$m$是整个训练集的大小，$m_{mini}$表示一个小批量数据集的大小，那么在小批量梯度下降中，算法每一次会利用$m_{mini}$条数据进行参数的更新：</p><script type="math/tex; mode=display">\begin{align}loss^t = 1/m_{mini}\cdot\zeta(\hat Y^t, Y^t)\end{align}</script><p>$t$表示批次</p><p>小批量梯度下降算法每次都会优化$loss^t$，但是对于整个训练集而已，$loss$并不一定呈现出优化的状态</p><p align="center"><img src="/2019/07/28/总结—2019-7-26/loss.png" height="200"></p></li></ul><p align="center"><font color="#333fff" size="2.8">图3: 损失函数</font></p><ul><li><p>Momentum(动量) gradient decent：在梯度下降中，我们是利用当前的$dW$来更新权值矩阵，momentum梯度下降则不同，它利用指数移动加权平均的思想，通过计算一个加权的$V_{dW}$来对权值矩阵进行更新：</p><script type="math/tex; mode=display">\begin{align}V_{dW} &= 0\\V_{dW_1} &= \beta V_{dW}+(1-\beta)dW_1\\&\cdots\\V_{dW_k} &= \beta V_{dW_{k-1}}+(1-\beta)dW_k\end{align}</script></li><li><p>RMSprop(均方根传递)：调整$dW$大小来加速优化过程：</p><script type="math/tex; mode=display">\begin{align}S_{dW} &= 0\\S_{dW_1} &= \beta S_{dW}+(1-\beta)dW_1^2\\&\cdots\\S_{dW_k} &= \beta S_{dW{k-1}}+(1-\beta)dW_{k-1}^2\end{align}</script><p>$W$的更新如下：$W = W-\alpha\frac {dW}{\sqrt {S_{dW}}}$</p></li><li><p>Adam：Adam算法融合了上面两种算法</p><script type="math/tex; mode=display">\begin{align}&V_{dW_k} = \beta_1V_{d_{W_{k-1}}} +(1-\beta_1)dW_k\\&S_{dW_k} = \beta_2 S_{dW{k-1}}+(1-\beta_2)dW_{k-1}^2\\&V_{dW_kcorr}=\frac{V_{dW_k}}{1-\beta_1^t}\\&S_{dW_kcorr}=\frac{S_{dW_k}}{1-\beta_2^t}\\\end{align}</script><p>$W$的更新如下：$W = W-\alpha\frac {V_{dW_kcorr}}{\sqrt {S_{dW_kcorr}}}$</p></li></ul><hr><ul><li>[ ] Q4.1：为什么mini-batch gradient dencent可以降低整个数据集上的损失函数？</li><li>[ ] Q4.2：mini-batch gradient dencent的噪声是由哪些因素决定的？这些因素又是怎样影响到噪声大小的？</li><li>[ ] Q4.3：上面提到的动量、RMSprop以及Adam加速优化的原因？</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;总结-2019-7-27&quot;&gt;&lt;a href=&quot;#总结-2019-7-27&quot; class=&quot;headerlink&quot; title=&quot;总结-2019/7/27&quot;&gt;&lt;/a&gt;总结-2019/7/27&lt;/h1&gt;&lt;p&gt;下面主要对&lt;strong&gt;7-22～7-26&lt;/strong&gt;这几天学习的一些知识进行一次回顾&lt;/p&gt;
    
    </summary>
    
    
      <category term="machien learning" scheme="http://yoursite.com/tags/machien-learning/"/>
    
  </entry>
  
  <entry>
    <title>numpy向量化操作</title>
    <link href="http://yoursite.com/2019/06/29/numpy%E5%90%91%E9%87%8F%E5%8C%96%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2019/06/29/numpy向量化操作/</id>
    <published>2019-06-29T00:48:14.000Z</published>
    <updated>2019-06-29T01:03:07.153Z</updated>
    
    <content type="html"><![CDATA[<h1 id="numpy向量化操作"><a href="#numpy向量化操作" class="headerlink" title="numpy向量化操作"></a>numpy向量化操作</h1><p>本文记录一些常见的利用numpy向量化操作提高运行速度的方法</p><a id="more"></a><h2 id="0-1矩阵的生成"><a href="#0-1矩阵的生成" class="headerlink" title="0\1矩阵的生成"></a>0\1矩阵的生成</h2><ul><li><p>在机器学习中，如果是二分类问题，最后需要将概率矩阵转化为0/1矩阵，如果使用for循环就没有办法利用到numpy的优势</p></li><li><p>下面是几个例子</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ypre</span><span class="params">(A, dim)</span>:</span> </span><br><span class="line">    y_pre = np.zeros((<span class="number">1</span>, dim))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>, i] &gt; <span class="number">0.5</span>:</span><br><span class="line">            y_pre[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> y_pre</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ypre_vectorize_1</span><span class="params">(A, dim)</span>:</span></span><br><span class="line">    y_pre = np.where(A &lt; <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> y_pre     </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ypre_vectorize_2</span><span class="params">(A, dim)</span>:</span></span><br><span class="line">    y_pre = np.floor(A + <span class="number">0.5</span>)</span><br><span class="line">    <span class="keyword">return</span> y_pre</span><br></pre></td></tr></table></figure></li><li><p><code>get_ypre</code>函数利用for循环遍历需要转换为0/1矩阵的A矩阵，<code>get_ypre_vectorize_1</code>和<code>get_ypre_vectorize_2</code>分别利用numpy的<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html" target="_blank" rel="noopener">np.where</a>和<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.floor.html" target="_blank" rel="noopener">np.floor</a>进行向量化处理，运行时间如下所示</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dim = <span class="number">10</span>**<span class="number">7</span></span><br><span class="line">A = np.random.rand(<span class="number">1</span>, dim)</span><br><span class="line">timeit(<span class="keyword">lambda</span>:get_ypre(A, dim), number=<span class="number">1</span>)</span><br><span class="line">out : <span class="number">2.667440585035365</span></span><br><span class="line">timeit(<span class="keyword">lambda</span>:get_ypre_vectorize_1(A, dim), number=<span class="number">1</span>)</span><br><span class="line">out : <span class="number">0.06458032497903332</span></span><br><span class="line">timeit(<span class="keyword">lambda</span>:get_ypre_vectorize_1(A, dim), number=<span class="number">1</span>)</span><br><span class="line">out : <span class="number">0.06309162796242163</span></span><br></pre></td></tr></table></figure></li><li><p>可以看到提升效果比较明显</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;numpy向量化操作&quot;&gt;&lt;a href=&quot;#numpy向量化操作&quot; class=&quot;headerlink&quot; title=&quot;numpy向量化操作&quot;&gt;&lt;/a&gt;numpy向量化操作&lt;/h1&gt;&lt;p&gt;本文记录一些常见的利用numpy向量化操作提高运行速度的方法&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>sklearn common usage</title>
    <link href="http://yoursite.com/2019/06/23/sklearn-common-usage/"/>
    <id>http://yoursite.com/2019/06/23/sklearn-common-usage/</id>
    <published>2019-06-23T01:41:02.000Z</published>
    <updated>2019-06-24T07:11:33.899Z</updated>
    
    <content type="html"><![CDATA[<h1 id="sklearn-common-usage"><a href="#sklearn-common-usage" class="headerlink" title="sklearn common usage"></a>sklearn common usage</h1><p>本文主要记录使用sklearn中遇到的一些常见用法</p><p><a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener">scikit-learn</a></p><h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a><a href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing" target="_blank" rel="noopener">Preprocessing</a></h2><ul><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer" target="_blank" rel="noopener">SimpleImputer</a></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line">imp_mean = SimpleImputer(missing_values=np.nan, strategy=<span class="string">'mean'</span>)</span><br><span class="line">imp_mean.fit_transform([[<span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, np.nan, <span class="number">6</span>], [<span class="number">10</span>, <span class="number">5</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure></li></ul></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder" target="_blank" rel="noopener">OneHotEncoder</a>&amp;<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder" target="_blank" rel="noopener">LabelEncoder</a></p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import OneHotEncoder</span><br><span class="line">enc = OneHotEncoder(handle_unknown=&apos;ignore&apos;)</span><br><span class="line">X = [[&apos;Male&apos;, 1], [&apos;Female&apos;, 3], [&apos;Female&apos;, 2]]</span><br><span class="line">enc.fit_transform(X)</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import LabelEncoder</span><br><span class="line">le = LabelEncoder()</span><br><span class="line">le.fit_transform([1, 2, 2, 6])</span><br></pre></td></tr></table></figure></li><li><p>一般在使用onehot之前先使用labelencoder进行处理，见<a href="https://towardsdatascience.com/encoding-categorical-features-21a2651a065c" target="_blank" rel="noopener">例子1</a>,<a href="https://www.ritchieng.com/machinelearning-one-hot-encoding/" target="_blank" rel="noopener">例子2</a></p></li></ul></li></ul><h2 id="Model-selection-and-evaluation"><a href="#Model-selection-and-evaluation" class="headerlink" title="Model selection and evaluation"></a><a href="https://scikit-learn.org/stable/model_selection.html#model-selection" target="_blank" rel="noopener">Model selection and evaluation</a></h2><ul><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">train_test_split</a></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" target="_blank" rel="noopener">StandardScaler</a></p><ul><li><blockquote><p>The standard score of a sample <a href="https://scikit-learn.org/stable/glossary.html#term-x" target="_blank" rel="noopener">x</a> is calculated as:</p><p>z = (x - u) / s</p></blockquote></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit_transform(X)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a><a href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning" target="_blank" rel="noopener">Regression</a></h2><ul><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" target="_blank" rel="noopener"><code>LinearRegression</code></a></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">reg = LinearRegression().fit(X, y)</span><br><span class="line">reg.predict(np.array([[<span class="number">3</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure></li><li></li></ul></li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;sklearn-common-usage&quot;&gt;&lt;a href=&quot;#sklearn-common-usage&quot; class=&quot;headerlink&quot; title=&quot;sklearn common usage&quot;&gt;&lt;/a&gt;sklearn common usage&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Stationarity of time series</title>
    <link href="http://yoursite.com/2019/06/15/Stationarity-of-time-series/"/>
    <id>http://yoursite.com/2019/06/15/Stationarity-of-time-series/</id>
    <published>2019-06-15T02:22:40.000Z</published>
    <updated>2019-06-15T03:01:26.275Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Stationarity-of-time-series"><a href="#Stationarity-of-time-series" class="headerlink" title="Stationarity of time series"></a>Stationarity of time series</h1><p>本文主要记录学习过程中关于时间序列平稳性的一些相关内容，包括平稳性的定义以及检测</p><h2 id="平稳性定义"><a href="#平稳性定义" class="headerlink" title="平稳性定义"></a>平稳性定义</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/06/15/Stationarity-of-time-series/平稳性.png" alt="平稳性" title="">                </div>                <div class="image-caption">平稳性</div>            </figure><p>在实际应用过程中，我们一般通过弱平稳来检测平稳性</p><p>上图来自<a href="https://www.zhihu.com/question/21982358" target="_blank" rel="noopener">知乎</a></p><h2 id="平稳性检测"><a href="#平稳性检测" class="headerlink" title="平稳性检测"></a>平稳性检测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.tsa.stattools <span class="keyword">import</span> adfuller</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_stationarity</span><span class="params">(timeseries)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Determing rolling statistics</span></span><br><span class="line">    rolmean = timeseries.rolling(<span class="number">12</span>).mean()</span><br><span class="line">    rolstd = timeseries.rolling(<span class="number">12</span>).std()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Plot rolling statistics:</span></span><br><span class="line">    orig = plt.plot(timeseries, color=<span class="string">'blue'</span>,label=<span class="string">'Original'</span>)</span><br><span class="line">    mean = plt.plot(rolmean, color=<span class="string">'red'</span>, label=<span class="string">'Rolling Mean'</span>)</span><br><span class="line">    std = plt.plot(rolstd, color=<span class="string">'black'</span>, label = <span class="string">'Rolling Std'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    plt.title(<span class="string">'Rolling Mean &amp; Standard Deviation'</span>)</span><br><span class="line">    plt.show(block=<span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Perform Dickey-Fuller test:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'Results of Dickey-Fuller Test:'</span>)</span><br><span class="line">    dftest = adfuller(timeseries, autolag=<span class="string">'AIC'</span>)</span><br><span class="line">    dfoutput = pd.Series(dftest[<span class="number">0</span>:<span class="number">4</span>], index=[<span class="string">'Test Statistic'</span>,<span class="string">'p-value'</span>,<span class="string">'#Lags Used'</span>,<span class="string">'Number of Observations Used'</span>])</span><br><span class="line">    <span class="keyword">for</span> key,value <span class="keyword">in</span> dftest[<span class="number">4</span>].items():</span><br><span class="line">        dfoutput[<span class="string">'Critical Value (%s)'</span>%key] = value</span><br><span class="line">    <span class="keyword">print</span> (dfoutput)</span><br><span class="line"></span><br><span class="line">test_stationarity(ts)</span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/06/15/Stationarity-of-time-series/平稳性检测.png" alt="平稳性检测" title="">                </div>                <div class="image-caption">平稳性检测</div>            </figure><p>利用adfuller检测时间序列平稳性，当Test Statistic小于某个Critical Value时则认为在该置信下认为时间序列平稳</p><h2 id="时间序列平稳化"><a href="#时间序列平稳化" class="headerlink" title="时间序列平稳化"></a>时间序列平稳化</h2><p><a href="https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/" target="_blank" rel="noopener">差分&amp;去除趋势</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Stationarity-of-time-series&quot;&gt;&lt;a href=&quot;#Stationarity-of-time-series&quot; class=&quot;headerlink&quot; title=&quot;Stationarity of time series&quot;&gt;&lt;/a&gt;Stati
      
    
    </summary>
    
    
      <category term="Statics" scheme="http://yoursite.com/tags/Statics/"/>
    
  </entry>
  
  <entry>
    <title>pandas common usage</title>
    <link href="http://yoursite.com/2019/06/10/pandas-common-usage/"/>
    <id>http://yoursite.com/2019/06/10/pandas-common-usage/</id>
    <published>2019-06-10T11:24:30.000Z</published>
    <updated>2019-06-29T01:07:32.697Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pandas-common-usage"><a href="#pandas-common-usage" class="headerlink" title="pandas common usage"></a>pandas common usage</h1><blockquote><p><em>pandas</em> is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the <a href="https://www.python.org/" target="_blank" rel="noopener">Python</a> programming language.</p><p><a href="https://pandas.pydata.org" target="_blank" rel="noopener">pandas</a></p></blockquote><p>本文主要记录在实践中遇到的一些常见的用法</p><h2 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h2><ul><li>df表示<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" target="_blank" rel="noopener">pandas.DataFrame</a></li></ul><h2 id="dataFrame"><a href="#dataFrame" class="headerlink" title="dataFrame"></a>dataFrame</h2><ul><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html#pandas.DataFrame.rolling" target="_blank" rel="noopener">rolling</a></p><ul><li><blockquote><p>为了提升数据的准确性，将某个点的取值扩大到包含这个点的一段区间，用区间来进行判断，这个区间就是窗口。移动窗口就是窗口向一端滑行，默认是从右往左</p></blockquote></li><li><p>rolling返回的类可以进行很多数值操作，例如<code>mean(), std(), sum()</code>,这些均是dataFrame包含的方法</p></li></ul></li><li><p><a href="https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.dropna.html#pandas.DataFrame.dropna" target="_blank" rel="noopener">dropna</a></p><ul><li>去除空值</li></ul></li><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html" target="_blank" rel="noopener">Merge, join, and concatenate</a></p><ul><li>对多个dataframe进行合并操作，包括行、列数据</li></ul></li><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html" target="_blank" rel="noopener">duplicated</a></p><ul><li>判断重复数据</li></ul></li><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html" target="_blank" rel="noopener">drop_duplicates</a></p><ul><li>​    去除重复数据</li></ul></li><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html" target="_blank" rel="noopener">group_by</a></p><ul><li>根据某些col对数据进行聚合</li></ul></li><li><p><del><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.as_matrix.html#pandas.DataFrame.as_matrix" target="_blank" rel="noopener">as_matrix</a></del></p><ul><li>转化为numpy的数组，注意columns参数是列名的列表</li></ul></li><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html#pandas.DataFrame.columns" target="_blank" rel="noopener">columns</a></p><ul><li>获取列名</li></ul></li><li><p>获取某个位置的元素</p><ul><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iat.html#pandas.DataFrame.iat" target="_blank" rel="noopener"><code>DataFrame.iat</code></a></p><p>Fast integer location scalar accessor.</p></li><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc" target="_blank" rel="noopener"><code>DataFrame.loc</code></a></p><p>Purely label-location based indexer for selection by label.</p></li><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.iloc.html#pandas.Series.iloc" target="_blank" rel="noopener"><code>Series.iloc</code></a></p><p>Purely integer-location based indexing for selection by position.</p></li></ul></li><li><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" target="_blank" rel="noopener">values</a></p><ul><li>返回numpy 数组值</li></ul></li><li><p><a href="[http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html#pandas.DataFrame.apply](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html#pandas.DataFrame.apply">apply</a>)</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.apply(np.sum, axis=0)</span><br><span class="line">df.apply(lambda x: [1, 2], axis=1)</span><br><span class="line">X[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col))</span><br></pre></td></tr></table></figure></li></ul></li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;pandas-common-usage&quot;&gt;&lt;a href=&quot;#pandas-common-usage&quot; class=&quot;headerlink&quot; title=&quot;pandas common usage&quot;&gt;&lt;/a&gt;pandas common usage&lt;/h1&gt;&lt;bloc
      
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Josephus Problem</title>
    <link href="http://yoursite.com/2019/01/05/Josephus-Problem/"/>
    <id>http://yoursite.com/2019/01/05/Josephus-Problem/</id>
    <published>2019-01-05T06:42:32.000Z</published>
    <updated>2019-01-12T03:13:33.218Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Josephus-Problem"><a href="#Josephus-Problem" class="headerlink" title="Josephus Problem"></a>Josephus Problem</h1><h2 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h2><blockquote><p>In <a href="https://en.wikipedia.org/wiki/Computer_science" target="_blank" rel="noopener">computer science</a> and <a href="https://en.wikipedia.org/wiki/Mathematics" target="_blank" rel="noopener">mathematics</a>, the <strong>Josephus problem</strong> (or <strong>Josephus permutation</strong>) is a theoretical problem related to a certain <a href="https://en.wikipedia.org/wiki/Counting-out_game" target="_blank" rel="noopener">counting-out game</a>.</p><p>People are standing in a <a href="https://en.wikipedia.org/wiki/Circle" target="_blank" rel="noopener">circle</a> waiting to be executed. Counting begins at a specified point in the circle and proceeds around the circle in a specified direction. After a specified number of people are skipped, the next person is executed. The procedure is repeated with the remaining people, starting with the next person, going in the same direction and skipping the same number of people, until only one person remains, and is freed.</p><p>The problem — given the number of people, starting point, direction, and number to be skipped — is to choose the position in the initial circle to avoid execution</p><p><a href="https://en.wikipedia.org/wiki/Josephus_problem" target="_blank" rel="noopener">more info</a></p><p>约瑟夫是公元一世纪著名的历史学家。在罗马人占领乔塔帕特后，39 个犹太人与约瑟夫及他的朋友躲到一个洞中，39个犹太人决定宁愿死也不要被敌人俘虏，于是决定了一个流传千古的自杀方式，41个人排成一个圆圈，由第1个人开始报数，每报到第3人该人就必须自杀，然后再由下一个人重新报数，直到所有人都自杀身亡为止。</p></blockquote><p>Josephus problem是一个非常有意思的问题，理解起来很简单，但是要解决却不是太容易。以前在程序课上接触过这个问题，当时的解决方法非常暴力，直接模拟整个游戏过程来得到最终答案。最近读到《具体数学》再次接触到了这个问题，书中的解法确有让人拍案而起的欲望。下面从该问题最基础的类型说起，一步一步解释书中的解法。</p><h2 id="基础问题"><a href="#基础问题" class="headerlink" title="基础问题"></a>基础问题</h2><ul><li><p>现有一个n节点的圆圈，且从1到n有序排列。从1开始计数，每隔一个节点去除一个节点，问最后剩下的节点（下面简称幸存者）序号是多少？</p><p><img src="/2019/01/05/Josephus-Problem/josephus example.png" alt="josephus example"></p><p style="background-color:powderblue;font-size:15px;">Figure 1 </p></li><li><p>要解决这个问题，可以从递归的角度来考虑，这一点在图1中有很直观的表达。针对我们的问题，我们要求解的是幸存者序号，不妨表示为$f(n)$，根据n的奇偶性可以得到下面的关系</p></li></ul><script type="math/tex; mode=display">\begin{align}  问题1  \begin{cases}  f(1) &=& 1\\  f(2n) &=& 2·f(n)-1\\  f(2n+1) &=& 2·f(n)+1\\  \end{cases}  \end{align}</script><p>  现在的问题是如何根据上面的关系求出$f(n)$的表达式。首先不妨从一些简单的例子观察一下规律</p><div class="table-container"><table><thead><tr><th>n</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th></tr></thead><tbody><tr><td>$f$</td><td>1</td><td>1</td><td>2</td><td>1</td><td>2</td><td>3</td><td>4</td><td>1</td></tr></tbody></table></div><p>  看起来是有一定规律的，写成下面这样也许更清晰</p><div class="table-container"><table><thead><tr><th>n</th><th>$2^0$</th><th>$2^1$</th><th>$2^1+1$</th><th>$2^2$</th><th>$2^2+1$</th><th>$2^2+2$</th><th>$2^2+3$</th><th>$2^3$</th></tr></thead><tbody><tr><td>$f$</td><td>1</td><td>1</td><td>3</td><td>1</td><td>3</td><td>5</td><td>7</td><td>1</td></tr></tbody></table></div><p>  简单的归纳就是$f(n) = 2l+1, n=2^m+l$，下面用数学归纳法来证明这一结论</p><script type="math/tex; mode=display">  \begin{align}  &需要证明：对于n=2^m+l, f(n)=2l+1,其中0=<l<2^m\\  &证明：\\  &m = 0, n = 1\ 时f(1) = 2·0+1,结论成立\\  &假设m=k时结论成立,下正m=k+1时结论仍然成立\\  &m=k+1时, 2^{k+1}=<n<2^{k+2}\\  &当n为偶数时, n = 2^{k+1} + l, l为偶数, l=2l^*\\  &f(n) = 2f(2^k+l^*)-1=2(2l^*+1)-1=2l+1\\  &n为奇数时类似,不难得出当m=k+1时结论仍成立，得证  \end{align}</script><p>  现在我们已经有了$f(n)$的显式表达，不过更令人兴奋的是我们可以用一种更为简洁美观的方式来表达这一结果。</p><script type="math/tex; mode=display">\begin{align}  &首先将n=2^m+l表示为二进制的形式, n=(b_mb_{m-1}...b_1b_0)_2,即n=b_m2^m+..+b_0\\  &其中b_m=1, l=(b_{m-1}b_{m-2}...b1b0)_2\\  &2l+1=2(b_{m-1}b_{m-2}...b1b0)_2+1=(b_{m-1}b_{m-2}...b_1b_0b_m)_2\\  &f((b_mb_{m-1}...b_1b_0)_2)=(b_{m-1}b_{m-2}...b_1b_0b_m)_2\\  &这样计算f(n)就可以用计算机最喜欢的移位运算了  \end{align}</script><p>  第一次看到上面的结果，我确实很开心，感觉这种形式很漂亮。不过让我更惊喜的是不仅在基础情况下，在复杂的情况下我们也可以利用上面的形式来求解递归式。</p><p>为了叙述简便，把递归式中$f(dn)$ 中d称为step，下面一般化以d为基础</p><h2 id="问题一般化"><a href="#问题一般化" class="headerlink" title="问题一般化"></a>问题一般化</h2><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step=2"></a>Step=2</h3><ul><li>step=2与上面解决的问题有很大重合，问题形式化表达如下<script type="math/tex; mode=display">\begin{align}问题2  \begin{cases}  f(1) &=& \alpha\\  f(2n+j) &=& 2·f(n)+\beta_j \ \ (j=0, 1)  \end{cases}  \end{align}</script>问题2与问题1的唯一区别是对$\alpha,\beta_j$进行了一般化，在问题1中$\alpha=1,\beta_0=-1, \beta_1=1$</li></ul><script type="math/tex; mode=display">  \begin{align}  &首先将n=2^m+l表示为二进制的形式, n=(b_mb_{m-1}...b_1b_0)_2,即n=b_m2^m+..+b_0\\  &n = 1, f(1) = \alpha, f((1)_2) = \alpha\\  &对j进行分类\\  &j=0时l=2·l^*\\  &f(n)=2f(2^{m-1}+l*)+\beta_0\\  &f((b_mb_{m-1}...b_1b_0)_2)=2f((b_mb_{m-1}...b_1)_2)+\beta_0\\  &f((b_mb_{m-1}...b_1b_0)_2)=2f(b_mb_{m-1}...b_1)_2)+\beta_{b_0}\\  &j=1  时类似，递归可以得到如下结果\\  &f((b_mb_{m-1}...b_1b_0)_2)=2^m\alpha+2^{m-1}\beta_{b_{m-1}}+...+\beta_{b_0}\\  &f((b_mb_{m-1}...b_1b_0)_2)=(\alpha\beta_{b_{m-1}}...\beta_{b_0})_2  \end{align}</script><p>  可以看到，我们再次利用二进制形式写出了任意$f(n)$的解表达式。不过需要注意的是这里的二进制与一般意义上的二进制表达式有一定区别，这里在某个二进制位上的数值可能取0，1之外的值</p><h3 id="Step-d-d-gt-1"><a href="#Step-d-d-gt-1" class="headerlink" title="Step = d, d&gt;1"></a>Step = d, d&gt;1</h3><ul><li>这里对step一般化，取任意大于1的整数值，问题形式化表达如下<script type="math/tex; mode=display">\begin{align}问题3  \begin{cases}  f(j) &=& \alpha_j\ (0<j<d)\\   f(dn+j) &=& cf(n)+\beta_j \ \ (0=<j<d)  \end{cases}  \end{align}</script></li></ul><script type="math/tex; mode=display">\begin{align}&首先将n=2^m+l表示为d进制的形式, n=(b_mb_{m-1}...b_1b_0)_d,即n=b_md^m+..+b_0\\&n = j, f(j) = \alpha_j, f((j)_d) = \alpha_j, \ (0<j<d)\\&n=(b_mb_{m-1}...b_1b_0)_d, b_0=j\\&f((b_mb_{m-1}...b_1b_0)_d)=2f((b_mb_{m-1}...b_1)_2)+\beta_0\\&f((b_mb_{m-1}...b_1b_0)_d)=cf(b_mb_{m-1}...b_1)_d)+\beta_{b_0}\\&f((b_mb_{m-1}...b_1b_0)_d)=c^m\alpha_{b_m}+c^{m-1}\beta_{b_{m-1}}+...+\beta_{b_0}\\&f((b_mb_{m-1}...b_1b_0)_d)=(\alpha_{b_m}\beta_{b_{m-1}}...\beta_{b_0})_c\end{align}</script><ul><li>问题3是最一般化的形式，从最后结果来看，问题3中定义的递归表达式将一个d进制的数映射到了另一个c进        制的数</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Josephus-Problem&quot;&gt;&lt;a href=&quot;#Josephus-Problem&quot; class=&quot;headerlink&quot; title=&quot;Josephus Problem&quot;&gt;&lt;/a&gt;Josephus Problem&lt;/h1&gt;&lt;h2 id=&quot;问题引入&quot;&gt;&lt;a 
      
    
    </summary>
    
    
      <category term="Mathematics" scheme="http://yoursite.com/tags/Mathematics/"/>
    
      <category term="Concrete Mathematics" scheme="http://yoursite.com/tags/Concrete-Mathematics/"/>
    
      <category term="Recurrent Problem" scheme="http://yoursite.com/tags/Recurrent-Problem/"/>
    
  </entry>
  
  <entry>
    <title>BurnSide&#39;s Lemma</title>
    <link href="http://yoursite.com/2018/12/30/BurnSide-Lemma/"/>
    <id>http://yoursite.com/2018/12/30/BurnSide-Lemma/</id>
    <published>2018-12-30T00:35:41.000Z</published>
    <updated>2019-01-05T08:01:32.631Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Burnside’s-lemma"><a href="#Burnside’s-lemma" class="headerlink" title="Burnside’s lemma"></a>Burnside’s lemma</h1><h2 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h2><ul><li><p>利用三种颜色给一个正三边形的顶点着色，在考虑图案重复的情况下总共有多少不同的着色方法？</p></li><li><p>上面的着色问题是一类计数问题，在计数的过程中由于需要考虑到去除重复的元素，所以问题会显得比较复杂</p><p><img src="/2018/12/30/BurnSide-Lemma/colour_symmetries.png"></p><p style="background-color:powderblue;font-size:15px;">Figure 1 </p><p>以上图为例，六种三角形的着色实际是等价的，只能算一种着色方式。</p></li></ul><h2 id="Burnside’s-lemma定义"><a href="#Burnside’s-lemma定义" class="headerlink" title="Burnside’s lemma定义"></a>Burnside’s lemma定义</h2><ul><li><blockquote><p><strong>Burnside’s lemma</strong>, sometimes also called <strong>Burnside’s counting theorem</strong>, the <strong>Cauchy–Frobenius lemma</strong>,<strong>orbit-counting theorem</strong>, or <strong>The Lemma that is not Burnside’s</strong> , is a result in <a href="https://en.wikipedia.org/wiki/Group_theory" target="_blank" rel="noopener">group theory</a> which is often useful in taking account of <a href="https://en.wikipedia.org/wiki/Symmetry" target="_blank" rel="noopener">symmetry</a> when counting mathematical objects.In the following, let <em>G</em> be a <a href="https://en.wikipedia.org/wiki/Finite_set" target="_blank" rel="noopener">finite</a> <a href="https://en.wikipedia.org/wiki/Group_(mathematics" target="_blank" rel="noopener">group</a>) that <a href="https://en.wikipedia.org/wiki/Group_action" target="_blank" rel="noopener">acts</a> on a <a href="https://en.wikipedia.org/wiki/Set_(mathematics" target="_blank" rel="noopener">set</a>) <em>X</em>. For each <em>g</em> in <em>G</em> let <em>Xg</em> denote the set of <a href="https://en.wikipedia.org/wiki/Element_(mathematics" target="_blank" rel="noopener">elements</a>) in <em>X</em> that are <a href="https://en.wikipedia.org/wiki/Fixed_point_(mathematics" target="_blank" rel="noopener">fixed by</a>) <em>g</em> (also said to be left <a href="https://en.wikipedia.org/wiki/Invariant_(mathematics" target="_blank" rel="noopener">invariant</a>) by <em>g</em>), i.e. <em>$X^g $</em> = { <em>x</em> ∈ <em>X</em> | <em>g</em>.<em>x</em> = <em>x</em> }. Burnside’s lemma asserts the following formula for the number of <a href="https://en.wikipedia.org/wiki/Orbit_(group_theory" target="_blank" rel="noopener">orbits</a>), denoted |<em>X</em>/<em>G</em>|</p>$$|X/G|={\frac  {1}{|G|}}\sum _{{g\in G}}|X^{g}|$$<p><a href="https://en.wikipedia.org/wiki/Burnside%27s_lemma" target="_blank" rel="noopener">Burnside’s lemma</a></p></blockquote></li><li><p>上面给出了Burnside’s lemma的定义，其具体的证明过程可参考给出的链接。利用Burnside’s lemma我们现在可以来试着解决一开始给出的着色问题了。</p></li></ul><h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><ul><li>着色问题的形式化定义<ul><li>$X :=\{不考虑重复的所有着色图案\}$</li><li>$G :=\{对任一图案的所有变换\}$</li><li>$|X/G| := 所有不重复的着色方式总数$</li></ul></li><li>$G$ 代表了对图案的变换方式，图1实际上就包含了对三角形的所有变换方式。其中旋转变换有三种，对称变换有三种。以旋转$\frac 2 3 \pi$为例，$X^g = \{x | colorA = colorB = colorC\}$  只有满足条件的着色图案才有$g.x=x$ ，不难计算这类着色方式有三种，$3^1$</li><li>对于上述的三色三角形问题，总共有 $\frac 1 6(3^3+2·3^1+3·3^2) = 10$种着手方式</li></ul><h2 id="问题一般化"><a href="#问题一般化" class="headerlink" title="问题一般化"></a>问题一般化</h2><ul><li><p>利用上述定理可以解决任意$(n, m)$着色问题，其中n代表正多边形的顶点数，m代表颜色数。</p></li><li><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2018/12/30/BurnSide-Lemma/question.png" alt="question" title="">                </div>                <div class="image-caption">question</div>            </figure><p style="background-color:powderblue;font-size:15px;">Figure 2 </p><p>图2便是一个一般化的正多边形的着色问题。</p></li><li><p>解决这个一般化问题可以从两个部分入手</p><ul><li><p>旋转变换对应的$X^g$: 对于n边形，旋转变换共有n种，旋转度数从$0-2\pi$,关键问题是如何计算每一种旋转变换的着色数量。下面尝试将问题形式化</p><script type="math/tex; mode=display">\begin{align}&对于任一旋转变换d(0=< d < n),顶点可分为以下几类\\&\Phi _0 = \{0, d, 2d, ...\}\\&\Phi _1 = \{1, d+1, 2d+1, ...\}\\&...\\&\Phi _i = \{i, d+i, 2d+i, ...\}\\&对于任一顶点x, x属于并仅属于某一\Phi\\&对任一(x_1,x_2)属于\Phi_j, x_1 = x_2+kd\ (mod\ n)\\&对于同一\Phi中的顶点，颜色应该一致，总的着色数有m^{i+1}\end{align}</script></li><li><p>对称变换较容易，考虑奇偶分类处理即可</p></li></ul></li><li><p>Python代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(n):</span><br><span class="line">    count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        count[i] = <span class="number">0</span></span><br><span class="line">    uncount, ucNow, ucAll = <span class="keyword">True</span>, <span class="number">0</span>, []</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        ucFirst, orbitSet = <span class="number">0</span>, []</span><br><span class="line">        <span class="keyword">while</span> ucFirst != n:</span><br><span class="line">            <span class="keyword">if</span> count[ucFirst] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ucFirst += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> ucFirst == n:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        i = ucFirst</span><br><span class="line">        <span class="keyword">while</span> count[i] != <span class="number">1</span>:</span><br><span class="line">            orbitSet.append(i)</span><br><span class="line">            count[i] = <span class="number">1</span></span><br><span class="line">            i = (i+step)%n </span><br><span class="line">        ucNow += <span class="number">1</span></span><br><span class="line">//模拟得出所有的orbitSet集合，例如n=<span class="number">10</span>，step=<span class="number">2</span>时，orbitSet集合为</span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>]</span><br><span class="line">[<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure><p>通过证明可以得到$|\Phi| = grd(n, d)$, 这样就不用计算每个集合的元素，直接可以得到集合个数</p><p>对称变换太简单，这里不详细说明。</p></li><li><p><a href="https://codeforces.com/blog/entry/51272" target="_blank" rel="noopener">其它类似问题</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Burnside’s-lemma&quot;&gt;&lt;a href=&quot;#Burnside’s-lemma&quot; class=&quot;headerlink&quot; title=&quot;Burnside’s lemma&quot;&gt;&lt;/a&gt;Burnside’s lemma&lt;/h1&gt;&lt;h2 id=&quot;问题引入&quot;&gt;&lt;a 
      
    
    </summary>
    
    
      <category term="Mathematics" scheme="http://yoursite.com/tags/Mathematics/"/>
    
      <category term="GroupTheory" scheme="http://yoursite.com/tags/GroupTheory/"/>
    
      <category term="CountProblem" scheme="http://yoursite.com/tags/CountProblem/"/>
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
</feed>
