{"meta":{"title":"Alephn's Blog","subtitle":"Stay hungry, stay foolish","description":null,"author":"Alephn","url":"http://yoursite.com"},"pages":[{"title":"tags","date":"2018-12-29T12:29:30.165Z","updated":"2018-12-29T12:27:08.999Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"STAT-156 L1-L2","slug":"STAT-156-L1-L2","date":"2019-07-31T13:25:46.000Z","updated":"2019-07-31T13:30:57.189Z","comments":true,"path":"2019/07/31/STAT-156-L1-L2/","link":"","permalink":"http://yoursite.com/2019/07/31/STAT-156-L1-L2/","excerpt":"STAT157 L1-L2笔记关于课程STAT157的学习笔记 学习时间2019/7/29","text":"STAT157 L1-L2笔记关于课程STAT157的学习笔记 学习时间2019/7/29 Logistics, Software, Linear Algebra 主要介绍相关软件以及线性代数的基础知识，由于我使用的设备不支持GPU，所以不考虑使用推荐的软件，关于线性代数的作业也比较基础，并且之前在其它课程中对向量、矩阵等运算已经有了不少训练，因此这一部分暂且略过 Probability and Statistics (Bayes Rule, Sampling Naive Bayes, Sampling) 朴素贝叶斯：课程中以垃圾邮件分类为例大致讲解了朴素贝叶斯的原理，但是非常简略，以至于我没有听懂。。。虽然之前已经学过朴素贝叶斯，但是很久没有接触导致忘的差不多了，下面是根据一篇教程对朴素贝叶斯的一个回顾 假设待分类项$x$具有特征$a_1, a_2, \\cdots, a_n$，（例如对于邮件而言可以把单词是否出现作为特征），$x$可能的分类有$y_1, y_2,\\cdots, y_m$，我们需要对给定的$x$确定最优分类$y_{max}$ 如何利用朴素贝叶斯的原理来解决上面的问题呢？ 首先看一下条件概率的计算 \\begin{align} P(A|B)&=\\frac{P(AB)}{P(B)}\\\\ &=\\frac{P(B|A)P(A)}{P(B)} \\end{align}将$B$看作是特征，$A$看作分类，我们要求的是$P(y=y_i|x=(a_1, a_2, \\cdots, a_n))$，因为对于同一个分类项$x$来说，$P(x=(a_1, a_2, \\cdots, a_n))$是一个常数，因此在计算不同可能分类的条件概率时我们不用考虑它，也就是说$P(y=y_i|x=(a_1, a_2, \\cdots, a_n))\\propto P(x=(a_1, a_2, \\cdots, a_n)|y=y_i)\\cdot P(y_i)$ 当认为各个特征选择完全独立时，$P(x=(a_1, a_2, \\cdots, a_n)|y=y_i)=P(x_1=a_1|y=y_i)\\cdots P(x_n=a_n|y=y_i)$ 即：$P(y=y_i|x=(a_1, a_2, \\cdots, a_n))\\propto P(x_1=a_1|y=y_i)\\cdots P(x_n=a_n|y=y_i)\\cdot P(y_i)$，也是课程中的说明。 有了上面的简要介绍，接下来我们可以针对具体问题来利用朴素贝叶斯来构建分类模型了。与课程一样，这里以MNIST手写数字为例，在MNIST数据集中，$x$可以看作一张图片的所有像素点，每个像素点的取值选择为0/1，$y$总共有十种，从0-9。由此，我们基本对问题定义有了大概的了解，接着需要利用训练集计算$P(x_i=1|y_j)$以及$P(y=y_j)$其中$ i=1～784, j=0～9$，$i$表示像素点的序号，对于测试样例，只要计算$P(x_1=a_1|y=y_j)\\cdots P(x_{784}=a_n|y=y_j)\\cdot P(y_j)$取得最大值的$j$即为预测结果。 具体的分类器构建可以参考STAT 156 Naive Bayes，为了加深对朴素贝叶斯的理解，我自己也构建了一个分类器，Naive Bayes Classifier 在自己实现分类器的过程中，我认为有两点需要注意，其一是$P(x_i=1|y_j)$计算时分母是$y_i$对应的统计数目而非整个训练集大小，其二可以认为是一个小track，在某些情况下$P(x_i=1|y_j)$可能为0，因为需要进行$log$操作，所以这时会报warning，为了避免，可以按照下面的方式初始化： 12xcount = np.ones((10, 28*28))ycount = np.ones((1, 10)) 大数定理&amp;中心极限定理 课程中其它部分主要涉及了概率论的基础知识，介绍了一些概率分布（笔记本中有涉及到），我认为能够掌握大数定理和中心极限定理，对理解课程中的内容有很大帮助，下面简要介绍 大数定理： 根据百度百科 伯努利大数定律：设$\\mu$是n次独立试验中事件A发生的次数，且事件A在每次试验中发生的概率为$p$，则对任意正数ε: $\\lim_{n\\to\\infty}P(|\\frac{\\mu_n}{n}-p|&lt;\\epsilon)=1$ 其含义是，当n足够大时，事件A出现的频率将几乎接近于其发生的概率，即频率的稳定性。 这解释了课程notebook中模拟多次掷骰子的实验结果 中心极限定理：根据《概率论与数理统计》陈希儒版 在概率论中，习惯于把和的分布收敛与正态分布的那一类定理叫做“中心极限定理”，下面的定理就是其中之一： \\begin{align} &设X_1, X_2,\\cdots X_n为独立同分布的随机变量，E(X_i)=a, Var(X_i)=\\sigma^2,0","categories":[],"tags":[{"name":"machine lerning,STAT157","slug":"machine-lerning-STAT157","permalink":"http://yoursite.com/tags/machine-lerning-STAT157/"}]},{"title":"STAT157 L3-L4","slug":"STAT157-L3-L4","date":"2019-07-31T00:57:19.000Z","updated":"2019-07-31T13:27:46.283Z","comments":true,"path":"2019/07/31/STAT157-L3-L4/","link":"","permalink":"http://yoursite.com/2019/07/31/STAT157-L3-L4/","excerpt":"STAT157 L3-L4笔记关于课程STAT157的学习笔记 学习时间2019/7/30 - 2019/7/31","text":"STAT157 L3-L4笔记关于课程STAT157的学习笔记 学习时间2019/7/30 - 2019/7/31 Gradients, Chain Rule, Automatic Differentiation这一节主要讲述梯度的运算技巧以及自动微分 课程内容 梯度运算 课程中关于各种梯度的运算可以总结为下面的一张图 图1:梯度运算总结 按照课程，常数对纵向量的梯度是横向量，这里让我比较疑惑，之前关于梯度的运算我费了很大功夫，虽然没有完全理解，但是基本能计算一些神经网络的反向传播过程，这里我觉得STAT157关于梯度运算讲解的不是很详细，可以参考Derivatives, Backpropagation, and Vectorization以及Vector, Matrix, and Tensor Derivatives 自动微分 自动微分区别于符号微分与数值微分，例如在神经网络的反向传播过程中，我们需要计算损失函数对于各个参数的梯度，然后根据公式来实现代码，这个过程需要一定的数学基础（我现在也只是大概能求解，Jacobian矩阵真的不是很好理解）。在很多深度学习框架中，例如tensorflow，以及本课程用到的MXnet都使用了自动微分技术，以达到向用户隐藏微分的细节。为了更好的说明自动微分，下面没有使用STAT课件中的例子，而是采纳了自动微分(Automatic Differentiation)简介 图2:Autodiff计算图 前向自动微分：图2表示的是一个2输入，1输出的计算图，在前向自动微分中，我们计算每个节点对输入的微分，整个过程从输入到输出，例如$v_0=f_1(x2), v_2=f_2(v1)$，$v_0^{‘}=f_1^{‘}, v_2^{‘}=f_2^{‘}\\cdot v_0^{‘}$ 反向自动微分：与前向自动微分不同，反向自动微分计算的是输出对每个节点的微分，这个过程从输出到输入，与反向传播过程类似。 自动微分(Automatic Differentiation)简介中有更加详细的介绍 Jupyter Notebook 在运行课程notebook时，下面的现象让我有些疑惑 123456x = mx.nd.array([[1, 2], [3, 4]])x.attach_grad()with autograd.record(): y = x * 2 #y.attach_grad() z = y * x 在运行上面的代码时，对于y是否进行attach_grad操作，得到的结果会不同，如果不进行attach_grad操作，x.grad 123[[ 4. 8.] [12. 16.]]&lt;NDArray 2x2 @cpu(0)&gt; 否则为： 123[[2. 4.] [6. 8.]]&lt;NDArray 2x2 @cpu(0)&gt; 猜测是，对y进行attach_grad操作后，计算过程为$\\frac {\\partial z}{\\partial x}=\\frac {\\partial z}{\\partial y}\\cdot\\frac {\\partial y}{\\partial x}$，但是程序计算$\\frac {\\partial z}{\\partial y}=x$，这其实是不正确的，利用符号微分： \\begin{align} \\frac {\\partial z}{\\partial y} &= \\frac {\\partial y}{\\partial y}\\cdot x+\\frac {\\partial x}{\\partial y}\\cdot y\\\\ &=x+\\frac 1 2\\cdot2 x\\\\ &=2x \\end{align}这里给我造成了一些疑惑，课程中并没有进行解释，暂且搁置 关于MXnet Autograd的更多内容，可以参考Autograd Package Linear Regression, Basic Optimization这一小节主要介绍了线性回归模型，并且讲述了如何利用MXnet来进行求解 课程内容 通过预测房价的例子介绍了线性回归模型的定义 介绍了mini-batch SGD 因为线性回归模型比较简单，并且我基本还记得，这里对于课程内容就不做过多记录 Jupyter Notebook 课程中介绍了两种方式实现mini-batch SGD，一种是自己一步一步构建模型，包括前向反馈以及反向传播过程，这与吴恩达老师的神经网络与深度学习中的练习基本类似，不过这里可以利用mxnet的自动微分来进行反向传播过程中的梯度运算，另一种是利用mxnet gluon来搭建模型 在Linear regression中利用上述两种方式完成了模型的构建及优化（数据部分采用课程Notebook） 关于mxnet gluon可以参考Create a neural network","categories":[],"tags":[{"name":"machine lerning","slug":"machine-lerning","permalink":"http://yoursite.com/tags/machine-lerning/"},{"name":"STAT157","slug":"STAT157","permalink":"http://yoursite.com/tags/STAT157/"}]},{"title":"总结—2019/7/27","slug":"总结—2019-7-26","date":"2019-07-28T09:42:06.000Z","updated":"2019-07-28T10:03:43.086Z","comments":true,"path":"2019/07/28/总结—2019-7-26/","link":"","permalink":"http://yoursite.com/2019/07/28/总结—2019-7-26/","excerpt":"总结-2019/7/27下面主要对7-22～7-26这几天学习的一些知识进行一次回顾","text":"总结-2019/7/27下面主要对7-22～7-26这几天学习的一些知识进行一次回顾 主要学习资料 吴恩达coursera课程 Derivatives, Backpropagation, and Vectorization Xavier and He Normal (He-et-al) Initialization 内容回顾part1：手工构建神经网络​ 在这一周，通过课程我学习了如何利用numpy等python库来搭建一个多层的神经网络，并且对其进行训练。多层神经网络的结构如下图所示： 图1:多层神经网络结构 ​ 为了构建这样一个多层的神经网络，大致需要经过以下几个步骤 参数的初始化：例如从输入到隐藏层，我们需要的是权值矩阵以及偏置，这在一开始就需要进行初始化。一般来说，初始化所有的参数，需要先定义出各层的规模。参数的初始化对之后的训练会起到一定影响，具体会在之后说明。 前向反馈过程：有了参数以及输入，我们需要实现上述神经网络的计算过程。以图一为例（参考其网络结构，但是更改输入为纵向量），整个网络的计算过程如下： Z_1 = W_1*X+b_1,\\ \\ \\ \\ \\ A_1 = f_1(Z_1)\\\\ Z_2 = W_2*A_1+b_2, \\ \\ \\ \\ A_2 = f_2(Z_2)\\\\ Z_3 = W_o*A_2+b_3,\\ \\ \\ \\ \\ \\ \\hat Y = f_3(Z_3)其中$f$表示激活函数，常见的有$relu, sigmoid$等，最后为了得到输出，一般会使用$sigmoid$或$softmax$，具体需要看是二分类还是多分类问题。 计算损失函数：上述神经网络最后可以计算得到预测结果$\\hat y$，以二分类问题为例，对数似然损失函数如下： loss = -[y*log(\\hat y)\\ +\\ (1-y)*log(1-\\hat y)]上面展示的是对一条数据的计算过程，当进行向量化处理时需要求均值。 反向传播过程：利用反向传播，我们可以计算出损失函数对各个参数的导数，之后对各参数进行更新以降低模型在训练数据上的损失，假设训练数据的规模为$m$，并且输出层为$sigmoid$，那么图1的反向传播可以表示如下： loss = -1/m*[Y*log(\\hat Y)\\ +\\ (1-Y)*log(1-\\hat Y)]\\\\ dZ_3 = \\hat Y\\ -\\ Y\\\\ \\ \\\\ dW_o = 1/m*dZ_3*A_2.T,\\ db_3 = 1/m*np.sum(dZ_3, axis=1)\\\\ dA_2 = W_o.T*dZ_3,\\ dZ_2 = dA_2*f_2^{'}(Z_2)\\\\ \\ \\\\ dW_2 = 1/m*dZ_2*A_1.T,\\ db_2 = 1/m*np.sum(dZ_2, axis=1)\\\\ dA_1 = W_2.T*dZ_2,\\ dZ_1 = dA_1*f_1^{'}(Z_1)\\\\ \\ \\\\ dW_1 = 1/m*dZ_1*A_1.T,\\ db_1 = 1/m*np.sum(dZ_1, axis=1) 模型的组合：利用上述的几个部分，可以定义出一个神经网络模型，在训练集上进行多次迭代，不断更新参数以降低损失函数，最后得到模型的参数 预测：利用学习得到的参数以及前向反馈过程就可以对测试集进行预测 part2：参数初始化在part1中已经提到模型的参数初始化会对模型训练产生影响，下面就对参数初始化进行具体介绍。 Zero Initialization 将所有的权值矩阵初始化为0，假设中间层的激活函数是$relu$，输出层的激活函数为$sigmoid$ \\begin{align} dW_1 &= dZ_1*X.T \\\\ &=dA_1*f^{'}(Z_1)*X.T\\\\ &=W_2.T*dZ_2*f^{'}(Z_1)*X.T \\end{align}可以看到在第一次迭代当中，$dW_1$受到$W_2$的影响结果为0，因此不会对$W_1$更新，类似的其它权值矩阵相同。对于$dW_L$: \\begin{align} dW_L &=dZ_L*A_{L-1}.T\\\\ &=(A_L-Y)*A_{L-1}.T \\end{align}因为中间层激活函数是$relu$，因此$dW_L$为0。如果激活函数是$tanh$，那么在第一轮迭代后会对$W_L$更新。但是由于权值矩阵中的每一行都是相同的（行向量可以看作是对输入的一个映射），根据对称性，在同一层的神经元作用相同，因此完全不推荐使用Zero Initialization Random Initialization 为了使每个神经元发挥不同的作用，我们需要保证权值矩阵中的每行向量具有区分度，因此我们可以利用Random Initialization来进行初始化。利用np.random.randn可以得到服从$N(\\mu=0, \\delta^2=1)$的特定$shape$的矩阵。但是初始化时权值矩阵的方差选择会成为比较棘手的问题，当选择过小时可能会出现梯度消失，导致训练速度过慢，当选择过大时可能会出现梯度爆炸，导致无法训练。 HE Normal Initialization 下面以中间层激活函数为$relu$的神经网络作为说明 \\begin{align} Z_1 &= W_1*X\\\\ A_1 &= relu(Z_1)\\\\ Z_{1_{ij}} &= \\sum_{k=0}^{k=nc}W_{ik}\\cdot X_{kj} \\end{align}$nc$表示$X$的特征数，或者$W_1$的列数。其中假设$W_1\\sim N(0, 1), X\\sim N(0, 1)$，利用基础的概率轮知识可以得到$Z_1\\sim N(0, nc)$，也就是说经过$W_1$后$Z_1$的方差变大了，为了尽可能优化后面的反向传播过程，这里将$W_1$的方差设置为$2/nc$，分子为2是因为采用的是$relu$函数，这种初始化的方法叫做：HE Normal Initialization [ ] Q2.1:关于这里的初始化我有一些不理解的地方，比如激活函数为$relu$时为什么分子选择2，$tanh$时选择分子为1？从$Z_1$到$A_1$，由于经过了一个非线性的映射，所以输出的分布完全变化了，在这时是否还是根据的方差或者均值来选择的分子呢？要解决这个疑问，后续可能需要阅读提出He-et-al的论文。 part3：正则化在机器学习的过程中，我们最终需要的是模型在测试集上取得优异的表现，但是在有些时候，模型可能会发生过拟合的情况，例如下图： 图2：Over-fitting $L_2$正则化 $loss = 1/m\\cdot \\zeta(\\hat Y,Y)+\\frac{\\lambda}{2m}\\cdot||\\theta||^2$ $\\frac{\\lambda}{2m}\\cdot||\\theta||^2$表示正则化项，将损失函数加上$L_2$正则化项后可以避免权值矩阵过大，以此可以降低过拟合 Dropout dropout，即随机失活，意味着在神经网络中随机对一些神经元进行失活操作，具体的操作需要一个失活矩阵$D$来实现： \\begin{align} Z_i &= W_i*A_{i-1}\\\\ A_i &= f(Z_i)\\\\ A_i &= A_i\\cdot D_i/keep_prob \\end{align}其中矩阵$D_i$是根据$keep_prob$来确定的0/1矩阵 [ ] Q3.1：$L_2$正则化有利于降低权值矩阵大小，但是权值矩阵的绝对大小对整个神经网络的效果会有什么样的影响呢？如何用数学化的形式具体说明$L_2$正则化是如何降低过拟合的呢？ [ ] Q3.2：在课程中，dropout是在激活层之后使用的，如果在激活层之前dropout会有什么区别呢？ [ ] Q3.3：dropout后保持输出的均值相同有什么作用 (利用$/keep_prob$保持均值) [ ] Q3.4：dropout是如何降低过拟合的？ part4：神经网络的优化Gradient descent，也就是梯度下降，是课程中一开始就使用的优化算法。根据维基百科: 梯度下降方法基于以下的观察：如果实值函数$\\displaystyle F(\\mathbf {x} )$在点$\\displaystyle \\mathbf {a} $处可微且有定义，那么函数$\\displaystyle F(\\mathbf {x} )$在$\\displaystyle \\mathbf {a} $点沿着梯度相反的方向 $\\displaystyle -\\nabla F(\\mathbf {a} )$ 下降最快。 因此，梯度下降通常也称为最速下降法。但是在数据规模很大时，梯度下降的成本非常高，因此需要进行一些转变 Mini-batch graillent decent: 小批量梯度下降，假设$m$是整个训练集的大小，$m_{mini}$表示一个小批量数据集的大小，那么在小批量梯度下降中，算法每一次会利用$m_{mini}$条数据进行参数的更新： \\begin{align} loss^t = 1/m_{mini}\\cdot\\zeta(\\hat Y^t, Y^t) \\end{align}$t$表示批次 小批量梯度下降算法每次都会优化$loss^t$，但是对于整个训练集而已，$loss$并不一定呈现出优化的状态 图3: 损失函数 Momentum(动量) gradient decent：在梯度下降中，我们是利用当前的$dW$来更新权值矩阵，momentum梯度下降则不同，它利用指数移动加权平均的思想，通过计算一个加权的$V_{dW}$来对权值矩阵进行更新： \\begin{align} V_{dW} &= 0\\\\ V_{dW_1} &= \\beta V_{dW}+(1-\\beta)dW_1\\\\ &\\cdots\\\\ V_{dW_k} &= \\beta V_{dW_{k-1}}+(1-\\beta)dW_k \\end{align} RMSprop(均方根传递)：调整$dW$大小来加速优化过程： \\begin{align} S_{dW} &= 0\\\\ S_{dW_1} &= \\beta S_{dW}+(1-\\beta)dW_1^2\\\\ &\\cdots\\\\ S_{dW_k} &= \\beta S_{dW{k-1}}+(1-\\beta)dW_{k-1}^2 \\end{align}$W$的更新如下：$W = W-\\alpha\\frac {dW}{\\sqrt {S_{dW}}}$ Adam：Adam算法融合了上面两种算法 \\begin{align} &V_{dW_k} = \\beta_1V_{d_{W_{k-1}}} +(1-\\beta_1)dW_k\\\\ &S_{dW_k} = \\beta_2 S_{dW{k-1}}+(1-\\beta_2)dW_{k-1}^2\\\\ &V_{dW_kcorr}=\\frac{V_{dW_k}}{1-\\beta_1^t}\\\\ &S_{dW_kcorr}=\\frac{S_{dW_k}}{1-\\beta_2^t}\\\\ \\end{align}$W$的更新如下：$W = W-\\alpha\\frac {V_{dW_kcorr}}{\\sqrt {S_{dW_kcorr}}}$ [ ] Q4.1：为什么mini-batch gradient dencent可以降低整个数据集上的损失函数？ [ ] Q4.2：mini-batch gradient dencent的噪声是由哪些因素决定的？这些因素又是怎样影响到噪声大小的？ [ ] Q4.3：上面提到的动量、RMSprop以及Adam加速优化的原因？","categories":[],"tags":[{"name":"machien learning","slug":"machien-learning","permalink":"http://yoursite.com/tags/machien-learning/"}]},{"title":"numpy向量化操作","slug":"numpy向量化操作","date":"2019-06-29T00:48:14.000Z","updated":"2019-06-29T01:03:07.153Z","comments":true,"path":"2019/06/29/numpy向量化操作/","link":"","permalink":"http://yoursite.com/2019/06/29/numpy向量化操作/","excerpt":"numpy向量化操作本文记录一些常见的利用numpy向量化操作提高运行速度的方法","text":"numpy向量化操作本文记录一些常见的利用numpy向量化操作提高运行速度的方法 0\\1矩阵的生成 在机器学习中，如果是二分类问题，最后需要将概率矩阵转化为0/1矩阵，如果使用for循环就没有办法利用到numpy的优势 下面是几个例子 1234567891011121314def get_ypre(A, dim): y_pre = np.zeros((1, dim)) for i in range(dim): if A[0, i] &gt; 0.5: y_pre[0, i] = 1 return y_predef get_ypre_vectorize_1(A, dim): y_pre = np.where(A &lt; 0.5, 0, 1) return y_pre def get_ypre_vectorize_2(A, dim): y_pre = np.floor(A + 0.5) return y_pre get_ypre函数利用for循环遍历需要转换为0/1矩阵的A矩阵，get_ypre_vectorize_1和get_ypre_vectorize_2分别利用numpy的np.where和np.floor进行向量化处理，运行时间如下所示 12345678dim = 10**7A = np.random.rand(1, dim)timeit(lambda:get_ypre(A, dim), number=1)out : 2.667440585035365timeit(lambda:get_ypre_vectorize_1(A, dim), number=1)out : 0.06458032497903332timeit(lambda:get_ypre_vectorize_1(A, dim), number=1)out : 0.06309162796242163 可以看到提升效果比较明显","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"sklearn common usage","slug":"sklearn-common-usage","date":"2019-06-23T01:41:02.000Z","updated":"2019-06-24T07:11:33.899Z","comments":true,"path":"2019/06/23/sklearn-common-usage/","link":"","permalink":"http://yoursite.com/2019/06/23/sklearn-common-usage/","excerpt":"","text":"sklearn common usage本文主要记录使用sklearn中遇到的一些常见用法 scikit-learn Preprocessing SimpleImputer 123from sklearn.impute import SimpleImputerimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')imp_mean.fit_transform([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]]) OneHotEncoder&amp;LabelEncoder 1234from sklearn.preprocessing import OneHotEncoderenc = OneHotEncoder(handle_unknown=&apos;ignore&apos;)X = [[&apos;Male&apos;, 1], [&apos;Female&apos;, 3], [&apos;Female&apos;, 2]]enc.fit_transform(X) 123from sklearn.preprocessing import LabelEncoderle = LabelEncoder()le.fit_transform([1, 2, 2, 6]) 一般在使用onehot之前先使用labelencoder进行处理，见例子1,例子2 Model selection and evaluation train_test_split 12from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) StandardScaler The standard score of a sample x is calculated as: z = (x - u) / s 123from sklearn.preprocessing import StandardScalerscaler = StandardScaler()scaler.fit_transform(X) Regression LinearRegression 123from sklearn.linear_model import LinearRegressionreg = LinearRegression().fit(X, y)reg.predict(np.array([[3, 5]]))","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"Stationarity of time series","slug":"Stationarity-of-time-series","date":"2019-06-15T02:22:40.000Z","updated":"2019-06-15T03:01:26.275Z","comments":true,"path":"2019/06/15/Stationarity-of-time-series/","link":"","permalink":"http://yoursite.com/2019/06/15/Stationarity-of-time-series/","excerpt":"","text":"Stationarity of time series本文主要记录学习过程中关于时间序列平稳性的一些相关内容，包括平稳性的定义以及检测 平稳性定义 平稳性 在实际应用过程中，我们一般通过弱平稳来检测平稳性 上图来自知乎 平稳性检测123456789101112131415161718192021222324from statsmodels.tsa.stattools import adfullerdef test_stationarity(timeseries): #Determing rolling statistics rolmean = timeseries.rolling(12).mean() rolstd = timeseries.rolling(12).std() #Plot rolling statistics: orig = plt.plot(timeseries, color='blue',label='Original') mean = plt.plot(rolmean, color='red', label='Rolling Mean') std = plt.plot(rolstd, color='black', label = 'Rolling Std') plt.legend(loc='best') plt.title('Rolling Mean &amp; Standard Deviation') plt.show(block=False) #Perform Dickey-Fuller test: print ('Results of Dickey-Fuller Test:') dftest = adfuller(timeseries, autolag='AIC') dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used']) for key,value in dftest[4].items(): dfoutput['Critical Value (%s)'%key] = value print (dfoutput)test_stationarity(ts) 平稳性检测 利用adfuller检测时间序列平稳性，当Test Statistic小于某个Critical Value时则认为在该置信下认为时间序列平稳 时间序列平稳化差分&amp;去除趋势","categories":[],"tags":[{"name":"Statics","slug":"Statics","permalink":"http://yoursite.com/tags/Statics/"}]},{"title":"pandas common usage","slug":"pandas-common-usage","date":"2019-06-10T11:24:30.000Z","updated":"2019-06-29T01:07:32.697Z","comments":true,"path":"2019/06/10/pandas-common-usage/","link":"","permalink":"http://yoursite.com/2019/06/10/pandas-common-usage/","excerpt":"","text":"pandas common usage pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. pandas 本文主要记录在实践中遇到的一些常见的用法 一些说明 df表示pandas.DataFrame dataFrame rolling 为了提升数据的准确性，将某个点的取值扩大到包含这个点的一段区间，用区间来进行判断，这个区间就是窗口。移动窗口就是窗口向一端滑行，默认是从右往左 rolling返回的类可以进行很多数值操作，例如mean(), std(), sum(),这些均是dataFrame包含的方法 dropna 去除空值 Merge, join, and concatenate 对多个dataframe进行合并操作，包括行、列数据 duplicated 判断重复数据 drop_duplicates ​ 去除重复数据 group_by 根据某些col对数据进行聚合 as_matrix 转化为numpy的数组，注意columns参数是列名的列表 columns 获取列名 获取某个位置的元素 DataFrame.iat Fast integer location scalar accessor. DataFrame.loc Purely label-location based indexer for selection by label. Series.iloc Purely integer-location based indexing for selection by position. values 返回numpy 数组值 apply) 123df.apply(np.sum, axis=0)df.apply(lambda x: [1, 2], axis=1)X[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col))","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Josephus Problem","slug":"Josephus-Problem","date":"2019-01-05T06:42:32.000Z","updated":"2019-01-12T03:13:33.218Z","comments":true,"path":"2019/01/05/Josephus-Problem/","link":"","permalink":"http://yoursite.com/2019/01/05/Josephus-Problem/","excerpt":"","text":"Josephus Problem问题引入 In computer science and mathematics, the Josephus problem (or Josephus permutation) is a theoretical problem related to a certain counting-out game. People are standing in a circle waiting to be executed. Counting begins at a specified point in the circle and proceeds around the circle in a specified direction. After a specified number of people are skipped, the next person is executed. The procedure is repeated with the remaining people, starting with the next person, going in the same direction and skipping the same number of people, until only one person remains, and is freed. The problem — given the number of people, starting point, direction, and number to be skipped — is to choose the position in the initial circle to avoid execution more info 约瑟夫是公元一世纪著名的历史学家。在罗马人占领乔塔帕特后，39 个犹太人与约瑟夫及他的朋友躲到一个洞中，39个犹太人决定宁愿死也不要被敌人俘虏，于是决定了一个流传千古的自杀方式，41个人排成一个圆圈，由第1个人开始报数，每报到第3人该人就必须自杀，然后再由下一个人重新报数，直到所有人都自杀身亡为止。 Josephus problem是一个非常有意思的问题，理解起来很简单，但是要解决却不是太容易。以前在程序课上接触过这个问题，当时的解决方法非常暴力，直接模拟整个游戏过程来得到最终答案。最近读到《具体数学》再次接触到了这个问题，书中的解法确有让人拍案而起的欲望。下面从该问题最基础的类型说起，一步一步解释书中的解法。 基础问题 现有一个n节点的圆圈，且从1到n有序排列。从1开始计数，每隔一个节点去除一个节点，问最后剩下的节点（下面简称幸存者）序号是多少？ Figure 1 要解决这个问题，可以从递归的角度来考虑，这一点在图1中有很直观的表达。针对我们的问题，我们要求解的是幸存者序号，不妨表示为$f(n)$，根据n的奇偶性可以得到下面的关系 \\begin{align} 问题1 \\begin{cases} f(1) &=& 1\\\\ f(2n) &=& 2·f(n)-1\\\\ f(2n+1) &=& 2·f(n)+1\\\\ \\end{cases} \\end{align} 现在的问题是如何根据上面的关系求出$f(n)$的表达式。首先不妨从一些简单的例子观察一下规律 n 1 2 3 4 5 6 7 8 $f$ 1 1 2 1 2 3 4 1 看起来是有一定规律的，写成下面这样也许更清晰 n $2^0$ $2^1$ $2^1+1$ $2^2$ $2^2+1$ $2^2+2$ $2^2+3$ $2^3$ $f$ 1 1 3 1 3 5 7 1 简单的归纳就是$f(n) = 2l+1, n=2^m+l$，下面用数学归纳法来证明这一结论 \\begin{align} &需要证明：对于n=2^m+l, f(n)=2l+1,其中0=","categories":[],"tags":[{"name":"Mathematics","slug":"Mathematics","permalink":"http://yoursite.com/tags/Mathematics/"},{"name":"Concrete Mathematics","slug":"Concrete-Mathematics","permalink":"http://yoursite.com/tags/Concrete-Mathematics/"},{"name":"Recurrent Problem","slug":"Recurrent-Problem","permalink":"http://yoursite.com/tags/Recurrent-Problem/"}]},{"title":"BurnSide's Lemma","slug":"BurnSide-Lemma","date":"2018-12-30T00:35:41.000Z","updated":"2019-01-05T08:01:32.631Z","comments":true,"path":"2018/12/30/BurnSide-Lemma/","link":"","permalink":"http://yoursite.com/2018/12/30/BurnSide-Lemma/","excerpt":"","text":"Burnside’s lemma问题引入 利用三种颜色给一个正三边形的顶点着色，在考虑图案重复的情况下总共有多少不同的着色方法？ 上面的着色问题是一类计数问题，在计数的过程中由于需要考虑到去除重复的元素，所以问题会显得比较复杂 Figure 1 以上图为例，六种三角形的着色实际是等价的，只能算一种着色方式。 Burnside’s lemma定义 Burnside’s lemma, sometimes also called Burnside’s counting theorem, the Cauchy–Frobenius lemma,orbit-counting theorem, or The Lemma that is not Burnside’s , is a result in group theory which is often useful in taking account of symmetry when counting mathematical objects.In the following, let G be a finite group) that acts on a set) X. For each g in G let Xg denote the set of elements) in X that are fixed by) g (also said to be left invariant) by g), i.e. $X^g $ = { x ∈ X | g.x = x }. Burnside’s lemma asserts the following formula for the number of orbits), denoted |X/G| $$|X/G|={\\frac {1}{|G|}}\\sum _{{g\\in G}}|X^{g}|$$ Burnside’s lemma 上面给出了Burnside’s lemma的定义，其具体的证明过程可参考给出的链接。利用Burnside’s lemma我们现在可以来试着解决一开始给出的着色问题了。 问题解决 着色问题的形式化定义 $X :=\\{不考虑重复的所有着色图案\\}$ $G :=\\{对任一图案的所有变换\\}$ $|X/G| := 所有不重复的着色方式总数$ $G$ 代表了对图案的变换方式，图1实际上就包含了对三角形的所有变换方式。其中旋转变换有三种，对称变换有三种。以旋转$\\frac 2 3 \\pi$为例，$X^g = \\{x | colorA = colorB = colorC\\}$ 只有满足条件的着色图案才有$g.x=x$ ，不难计算这类着色方式有三种，$3^1$ 对于上述的三色三角形问题，总共有 $\\frac 1 6(3^3+2·3^1+3·3^2) = 10$种着手方式 问题一般化 利用上述定理可以解决任意$(n, m)$着色问题，其中n代表正多边形的顶点数，m代表颜色数。 question Figure 2 图2便是一个一般化的正多边形的着色问题。 解决这个一般化问题可以从两个部分入手 旋转变换对应的$X^g$: 对于n边形，旋转变换共有n种，旋转度数从$0-2\\pi$,关键问题是如何计算每一种旋转变换的着色数量。下面尝试将问题形式化 \\begin{align} &对于任一旋转变换d(0=< d < n),顶点可分为以下几类\\\\ &\\Phi _0 = \\{0, d, 2d, ...\\}\\\\ &\\Phi _1 = \\{1, d+1, 2d+1, ...\\}\\\\ &...\\\\ &\\Phi _i = \\{i, d+i, 2d+i, ...\\}\\\\ &对于任一顶点x, x属于并仅属于某一\\Phi\\\\ &对任一(x_1,x_2)属于\\Phi_j, x_1 = x_2+kd\\ (mod\\ n)\\\\ &对于同一\\Phi中的顶点，颜色应该一致，总的着色数有m^{i+1} \\end{align} 对称变换较容易，考虑奇偶分类处理即可 Python代码 1234567891011121314151617181920212223for step in range(n): count = &#123;&#125; for i in range(n): count[i] = 0 uncount, ucNow, ucAll = True, 0, [] while True: ucFirst, orbitSet = 0, [] while ucFirst != n: if count[ucFirst] == 0: break else: ucFirst += 1 if ucFirst == n: break i = ucFirst while count[i] != 1: orbitSet.append(i) count[i] = 1 i = (i+step)%n ucNow += 1//模拟得出所有的orbitSet集合，例如n=10，step=2时，orbitSet集合为[0, 2, 4, 6, 8][1, 3, 5, 7, 9] 通过证明可以得到$|\\Phi| = grd(n, d)$, 这样就不用计算每个集合的元素，直接可以得到集合个数 对称变换太简单，这里不详细说明。 其它类似问题","categories":[],"tags":[{"name":"Mathematics","slug":"Mathematics","permalink":"http://yoursite.com/tags/Mathematics/"},{"name":"GroupTheory","slug":"GroupTheory","permalink":"http://yoursite.com/tags/GroupTheory/"},{"name":"CountProblem","slug":"CountProblem","permalink":"http://yoursite.com/tags/CountProblem/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://yoursite.com/tags/Algorithm/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]}]}