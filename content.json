{"meta":{"title":"Alephn's Blog","subtitle":"Stay hungry, stay foolish","description":null,"author":"Alephn","url":"http://yoursite.com"},"pages":[{"title":"tags","date":"2018-12-29T12:29:30.165Z","updated":"2018-12-29T12:27:08.999Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"STAT157 L10-L11","slug":"STAT157-L10-L11","date":"2019-08-14T02:24:48.000Z","updated":"2019-08-16T01:45:08.492Z","comments":true,"path":"2019/08/14/STAT157-L10-L11/","link":"","permalink":"http://yoursite.com/2019/08/14/STAT157-L10-L11/","excerpt":"STAT157 L10-L11笔记关于课程STAT157的学习笔记 学习时间2019/8/14","text":"STAT157 L10-L11笔记关于课程STAT157的学习笔记 学习时间2019/8/14 Layers, Parameters, GPUs这一节主要讲解了一些利用mxnet编程的知识，也可以参考MXnet tutorial Convolutional NetworksConvolutions Definition of Convolution \\begin{align} (f*g)(x)&=\\int_{-\\infty}^{\\infty}f(\\tau)\\cdot g(x-\\tau)d\\tau\\\\ (f*g)(x)&=\\sum_{-\\infty}^{\\infty}f(\\tau)\\cdot g(x-\\tau) \\end{align}上面是连续以及离散的情况下卷积的定义，需要注意的是积分变量是$\\tau$而不是$x$，关于如何理解卷积，可以参考如何通俗易懂地解释卷积和我对卷积的理解 Definition of Cross-correlation \\begin{align} (f\\star g)(x)&=\\int_{-\\infty}^{\\infty}f(\\tau)\\cdot g(x+\\tau)d\\tau\\\\ (f\\star g)(x)&=\\sum_{-\\infty}^{\\infty}f(\\tau)\\cdot g(x+\\tau) \\end{align}这里仅仅考虑实数，不涉及复数 Rethinking dense layer h_{i,j}=\\sum_{k, l}W_{i,j,k,l}X_{k,l}当处理图片时，假设输入的原始状态是二维的，为了保持隐层输出一致，我们可以利用四维的张量来进行变换，具体的公式如上，进行变量代换： \\begin{align} &h_{i,j}=\\sum_{a,b}V_{i,j,a,b}X_{i+a,j+b}\\\\ &W_{i,j,i+a,j+b}=V_{i,j,a,b} \\end{align}假设$V$与$i,j$独立，即$V_{i,j,a,b}=V_{a,b}$，那么: h_{i,j}=\\sum_{a,b}V_{a,b}X_{i+a,j+b}可以看到$h$是$V,X$的互相关函数 当对$X$进行变换得到$h_{i,j}$时，我们关注的区域不会离$X_{i,j}$太远，于是有： h_{i,j}=\\sum_{a=-\\Delta}^\\Delta \\sum_{b=-\\Delta}^\\Delta V_{a,b}X_{i+a,j+b}在$a,b$超出范围时令$V_{a,b}=0$ Convolution layer vs. Cross-correlation layer 相比于卷积运算，互相关更能发挥出计算机的cache机制优势 Padding and Stride padding：假设卷积核的尺寸为$k_h\\times k_w$，输入的尺寸为$n_h\\times n_w$，填充$p_h$行，$p_w$列，那么输出的尺寸为： (n_h-k_h+p_h+1)\\times (n_w-k_w+p_w+1)一般选择$p_h=k_h-1, p_w=k_w-1$，$k$为奇数，两边填充数为$\\frac {p}{2}$,$k$为偶数，一边填充$\\lceil \\frac p2\\rceil$，一边填充$\\lfloor\\frac p2\\rfloor$ strides：假设步长为$s_h,s_w$，那么输出尺寸为： \\lfloor (n_h-k_h+p_h+1)/s_h\\rfloor\\times \\lfloor(n_w-k_w+p_w+1)/s_w\\rfloor Channel channel $c$表示channel数，$B$表示bias Multiple input channels, single output channel $X: c_i\\times n_h\\times n_w$ $W: c_i\\times k_h\\times k_w$ $Y: m_h\\times m_w$ $Y=\\sum_{j=0}^{c_i}X_j\\star W_j+B_j$ Multiple input&amp;output channels $X: c_i\\times n_h\\times n_w$ $W: c_o\\times c_i\\times k_h\\times k_w$ $Y: c_o\\times m_h\\times m_w$ $Y_i=\\sum_{j=0}^{c_i}X_j\\star W_{i,j}+B_{i,j}$ pooling Why we need pooling? Rotational/Position Invariance Feature Extraction : Pooling can also be used for extracting rotational and position invariant feature. Consider the same example of using pooling of size 5x5. Pooling extracts the max value from the given 5x5 region. Basically extract the dominant feature value (max value) from the given region irrespective of the position of the feature value. The max value would be from any position inside the region. Pooling does not capture the position of the max value thus provides rotational/positional invariant feature extraction.","categories":[],"tags":[{"name":"machine lerning","slug":"machine-lerning","permalink":"http://yoursite.com/tags/machine-lerning/"},{"name":"STAT157","slug":"STAT157","permalink":"http://yoursite.com/tags/STAT157/"}]},{"title":"STAT157 L8-L9","slug":"STAT157-L8-L9","date":"2019-08-11T02:59:29.000Z","updated":"2019-08-12T04:56:10.417Z","comments":true,"path":"2019/08/11/STAT157-L8-L9/","link":"","permalink":"http://yoursite.com/2019/08/11/STAT157-L8-L9/","excerpt":"STAT157 L8-L9笔记关于课程STAT157的学习笔记 学习时间2019/8/10","text":"STAT157 L8-L9笔记关于课程STAT157的学习笔记 学习时间2019/8/10 Numerical Stability, HardwareNumerical Stability 在训练神经网络的过程中，需要注意数值稳定性问题，如梯度爆炸和梯度消失。 以MLP为例，假设前向反馈过程如下： \\begin{align} Z_1&=X*W_1+b_1\\\\ A_1&=\\sigma_1(Z_1)\\\\ &\\cdots\\\\ Z_i&=A_i*W_{i-1}+b_i\\\\ A_i&=\\sigma_i(Z_i)\\\\ &\\cdots\\\\ Z_n&=A_n*W_{n-1}+b_{n}\\\\ A_n&=\\sigma_n(Z_n) \\end{align}损失函数为$l=\\zeta(A_n, Y)$，将$\\frac{\\partial l}{\\partial p}$记做$dp$，那么有： \\begin{align} dA_n&=dA_n\\\\ dZ_n&=dA_n\\cdot\\dot{\\sigma_n}(Z_n)\\\\ dA_{n-1}&=dZ_n*W_n.T\\\\ &\\cdots\\\\ dA_i&=dZ_{i+1}*W_{i+1}.T\\\\ dZ_i&=dA_i\\cdot\\dot{\\sigma_i}(Z_i)\\\\ \\end{align}根据上面的偏导数计算公式，可以得到: \\begin{align} dW_i&=A_{i-1}.T*dZ_i\\\\ &=A_{i-1}.T*(dZ_{i+1}*W_{i+1}.T)\\cdot \\dot{\\sigma}_i(Z_i)\\\\ &=\\cdots\\\\ &=A_{i-1}.T*dA_n\\cdot \\dot{\\sigma}_n(Z_n)*W_n.T\\cdots \\dot{\\sigma}_{i+1}(Z_{i+1})*W_{i+1}.T\\cdot \\dot{\\sigma}_{i}(Z_i) \\end{align}参考The vanishing gradient problem，如果选择sigmoid函数，$\\dot{\\sigma}$小于1/4，$||W||$小于1，因此当隐藏层数增加，会出现梯度消失，当使用relu函数时，若权重初始化较大，则会出现梯度爆炸。 （这里我查找了一些资料，但是感觉都很粗略，并没有找到细致的证明） Weight Initialization 为了解决上面的问题，可以通过选择初始化的方法（缓解问题），可以参考总结—2019/7/27 之前关于激活函数对初始化的影响不是很理解，课程中对此给出了解释 利用泰勒展开就可以大致了解如何选择激活函数 Machine Learning Problems and Statistical EnvironmentDistribution ShiftCovariate Shift Here we assume that although the distribution of inputs may change over time, the labeling function, i.e., the conditional distribution $p(y|x)$ does not change. 上面的定义来自d2l，下面来介绍一个比较具体的例子进行解释(来自基于样例的迁移学习——Covariate Shift——原始文章解读) \\begin{align} train\\ data\\ distribution&:x\\sim N(0, 1)\\\\ test\\ data\\ distribution&:x\\sim N(0.5, 0.25)\\\\ conditional\\ distribution&:y|x\\sim N(-x+x^3, 0.1^2) \\end{align} 在d2l中，有提到一般假定Covariate shift的条件，即输入导致输出 when we believe x causes y, covariate shift is usually the right assumption to be working with. Label Shift The converse problem emerges when we believe that what drives the shift is a change in the marginal distribution over the labels p(y) but that the class-conditional distributions are invariant $p(x|y)$. 在d2l中，有提到一般假定label shift的条件，即输出导致输入，例如在医疗影像中判断病症，认为病症是引起特定影像的原因 Label shift is a reasonable assumption to make when we believe that y causes x Concept Shift One more related problem arises in concept shift, the situation in which the very label definitions change. 例如软饮料在不同地区的称呼不同 Covariate Shift Correction 假设测试集$x\\sim q(x)$，训练集$x\\sim p(x)$，真实的数据与label联合分布为$(x, y)\\sim p(x, y)=q(x)\\cdot p(y|x)$，在训练集中我们最小化的是 minimize_w\\frac1m\\sum_{i=1}^ml(f(x_i,w), y_i)+ some penalty(w)或者用积分的形式： minimize_w\\int p(x)dx\\int l(f(x,w),y)\\cdot p(y|x)dy但是在测试集中需要最小化的是： minimize_w\\int q(x)dx\\int l(f(x,w),y)\\cdot p(y|x)dy为了进行修正，根据下面的公式，我们需要找到一个概率密度比率函数$\\alpha(x)=\\frac{q(x)}{p(x)}$ \\begin{align} \\int q(x)f(x)dx&=\\int p(x)\\frac{q(x)}{p(x)}f(x)dx\\\\ &=\\int p(x)\\alpha(x)f(x)dx \\end{align}但是我们并不知道$\\alpha(x)$ 解决方法：利用训练集和测试集训练出一个分类器 当$x$来自训练集时$z=1$，否则$z=-1$，$r(z=1|x)=\\frac{p(x)}{p(x)+q(x)}$，所以$\\alpha(x)=\\frac{r(z=1|x)}{r(z=-1|x)}$ 利用logistic regression，$r(z=1|x)=\\frac{1}{1+exp(-f(x))}$，$\\alpha(x)=exp(f(x))$ 之后通过训练二分类器可得$f(x)$ 修正后的损失函数： minimize_w\\frac1m\\sum_{i=1}^ml(f(x_i,w), y_i)\\to minimize_w\\frac1m\\sum_{i=1}^ml(f(x_i,w), y_i)\\cdot exp(f(x_i)) Label Shift Correction 仅做了解，可以参考d2l Adversarial data slides Nonstationary Environments 环境","categories":[],"tags":[{"name":"machine lerning","slug":"machine-lerning","permalink":"http://yoursite.com/tags/machine-lerning/"},{"name":"STAT157","slug":"STAT157","permalink":"http://yoursite.com/tags/STAT157/"}]},{"title":"STAT157 L6-L7","slug":"STAT157-L6-L7","date":"2019-08-08T00:34:00.000Z","updated":"2019-08-11T03:00:17.585Z","comments":true,"path":"2019/08/08/STAT157-L6-L7/","link":"","permalink":"http://yoursite.com/2019/08/08/STAT157-L6-L7/","excerpt":"STAT157 L6-L7笔记关于课程STAT157的学习笔记 学习时间2019/8/08","text":"STAT157 L6-L7笔记关于课程STAT157的学习笔记 学习时间2019/8/08 Multilayer PerceptronSingle Layer Perceptron 单层感知机 需要注意的是单层感知机的训练算法，以及其算法的收敛性，这些在上面的链接中均有介绍 Multilayer Perceptron multilayer perceptron perceptron.png 上面的定义来自中文版动手学深度学习，若不做说明，下面的图都来自中文版动手学深度学习 Model Selection, Weight Decay, DropoutModel Selection 测试集&amp;验证集 在机器学习中，通常需要评估若干候选模型的表现并从中选择模型。这一过程称为模型选择 (model selection)。可供选择的候选模型可以是有着不同超参数的同类模型。以多层感知机为 例，我们可以选择隐藏层的个数，以及每个隐藏层中隐藏单元个数和激活函数。为了得到有效的 模型，我们通常要在模型选择上下一番功夫。下面，我们来描述模型选择中经常使用的验证数据 集(validation data set)。 模型复杂度估计 参数个数 参数取值范围 VC Dimension 数据复杂度估计 数据规模 数据特征数 数据结构 数据的多样性 Weight decay 权重衰减等价于L2 范数正则化(regularization)。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较 Drop out dropout","categories":[],"tags":[{"name":"machine lerning","slug":"machine-lerning","permalink":"http://yoursite.com/tags/machine-lerning/"},{"name":"STAT157","slug":"STAT157","permalink":"http://yoursite.com/tags/STAT157/"}]},{"title":"STAT157 L5","slug":"STAT157-L5","date":"2019-08-01T02:02:26.000Z","updated":"2019-08-08T00:23:42.499Z","comments":true,"path":"2019/08/01/STAT157-L5/","link":"","permalink":"http://yoursite.com/2019/08/01/STAT157-L5/","excerpt":"STAT157 L5笔记关于课程STAT157的学习笔记 学习时间2019/8/01","text":"STAT157 L5笔记关于课程STAT157的学习笔记 学习时间2019/8/01 Likelihood, Loss Functions, Logisitic Regression, Information Theory这一节的内容感觉学起来有些吃力，其中有不少内容是本科时候已经接触过多次的，比如极大似然，但是和之前的朴素贝叶斯STAT-157 L1-L2一样，由于缺乏思考和锻炼，掌握的一直很不牢靠。由于课程中对于这些知识讲解的也比较简略（毕竟默认是先修课程的内容），因此在下面的笔记中会引用到陈希儒教授的《概率论与数理统计》 Maximum Likelihood 什么是参数估计？ 设有一个统计总体，以$f(x; \\theta_1,\\cdots,\\theta_k)$记做其总体分布，具体含义视其是连续型还是离散型而定。这个分布包含了$k$个未知参数$\\theta_1,\\cdots,\\theta_k$，参数估计问题的一般提法是：设有了从总体中抽取出的独立同分布样本$X_1, X_2, \\cdots, X_n$，要依据这些样本去对$\\theta_1,\\cdots,\\theta_k$的未知值进行估计 什么是极大似然估计？ 极大似然估计，即Maximum likelihood estimation(MLE) 设总体分布为$f(x; \\theta_1,\\cdots,\\theta_k)$，那么样本$X_1, X_2, \\cdots, X_n$的分布（概率密度函数或者概率函数）为 $f(X_1; \\theta_1,\\cdots,\\theta_k)f(X_2; \\theta_1,\\cdots,\\theta_k)\\cdots f(X_n; \\theta_1,\\cdots,\\theta_k)$ 记做: $L(X_1, X_2,\\cdots, L_n; \\theta_1,\\cdots,\\theta_k)$ 固定$\\theta_1,\\cdots,\\theta_k$而看作$X_1, X_2, \\cdots, X_n$的函数，$L$是一个概率密度函数或概率函数，当把$X_1, X_2, \\cdots, X_n$固定而把$\\theta_1,\\cdots,\\theta_k$时称$L$为似然函数，此时$L$反映了在观察结果$X_1, X_2, \\cdots, X_n$已知条件下，$\\theta_1,\\cdots,\\theta_k$的各种值的”似然程度”，当满足条件： $L(X_1, X_2,\\cdots, L_n; \\theta_1^,\\cdots,\\theta_k^)=max_{\\theta_1,\\cdots,\\theta_k}L(X_1, X_2,\\cdots, L_n; \\theta_1,\\cdots,\\theta_k)$ 我们就用$\\theta_1^,\\cdots,\\theta_k^$作为未知参数的估计 一般而言，我们会通过求解$logL=\\sum_{i=1}^{n}logf(X_i;\\theta_1,\\cdots,\\theta_k)$的最大值得到$\\theta_1^,\\cdots,\\theta_k^$ 什么是最大后验估计？ 最大后验估计，即Maximum a posteriori estimation(MAP) 在极大似然估计中，我们最优化的目标是:$f(X_1; \\theta_1,\\cdots,\\theta_k)f(X_2; \\theta_1,\\cdots,\\theta_k)\\cdots f(X_n; \\theta_1,\\cdots,\\theta_k)$，也可以理解为$P(X|\\theta)$，我们认为$\\theta$是一个确定的未知值，但是在最大后验估计中，我们优化的是$P(\\theta|X)$，也就是需要最优化$P(X|\\theta)\\cdot P(\\theta)$ P(\\theta|X)=\\frac{P(X|\\theta)\\cdot P(\\theta)}{P(X)} Maximum Likelihood for Simple Linear Regression这一小节让我对线性回归有了一个新的认识！虽然一些细节还是不太理解，但是这里还是勉强进行一个总结 首先回顾一下线性回归问题：给定一系列数据$ (y_i,x_{i1},…,x_{ip})_{i=1}^n$，构建一个线性模型$\\hat y=WX+b$来对所有数据进行拟合。一般来说，我们会利用L2 loss来对这个问题进行求解，但是一直以来，好像都没有去思考为什么优化的的L2 loss？ 利用上面的定义，我们就能把线性回归和概率模型结合起来了（实际上维基百科就是这么定义的，只是这么多年学习线性回归时一直没有接触过。。。），我们最终优化的目标，其实是根据MAP（MLE）来确定的 上面就是在给定模型下，$(x, y)$出现的概率密度函数，由于参数未知，将模型写为： 似然函数L为： 最优解为： 当我们假设误差$\\epsilon ～N(0, \\sigma^2)$时，参数与最优化L2 loss的解相同。 课程中将上面似然函数的优化直接等价于(如下图)最优化L2 loss，这里我不是非常理解 上面的公式均出自36-401, Modern Regression, Section B Loss function很抱歉，这一节我基本没有看懂老师在说些什么。。。 loss 这张图里面的箭头含有究竟是指什么？或许这部分并不是很重要，暂且略过 logistic regression crossEntropy information theory Entropy：$H[p]=-\\sum_i(p_i\\cdot lnp_i)$，根据维基百科 在信息论中，熵（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。 Kullback-Leibler Divergence(KL散度) KL散度是两个概率分布P和Q差别的非对称性的度量。 KL散度是用来 度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。 维基百科 对于离散型随机变量，KL散度为：$D(P||Q)=-\\sum_i(P_i(\\frac{lnP_i}{lnQ_i}))$ \\begin{align} D(y||softmax(o))&=-\\sum_iy_ilny_i+\\sum_iy_ilnsoftmax(o)_i\\\\ &=H[y]+(y^To-\\sum_iexp(o_i)) \\end{align} 课程笔记 logistic regression","categories":[],"tags":[{"name":"machine lerning","slug":"machine-lerning","permalink":"http://yoursite.com/tags/machine-lerning/"},{"name":"STAT157","slug":"STAT157","permalink":"http://yoursite.com/tags/STAT157/"}]},{"title":"STAT-157 L1-L2","slug":"STAT-157-L1-L2","date":"2019-07-31T13:25:46.000Z","updated":"2019-07-31T13:45:27.138Z","comments":true,"path":"2019/07/31/STAT-157-L1-L2/","link":"","permalink":"http://yoursite.com/2019/07/31/STAT-157-L1-L2/","excerpt":"STAT157 L1-L2笔记关于课程STAT157的学习笔记 学习时间2019/7/29","text":"STAT157 L1-L2笔记关于课程STAT157的学习笔记 学习时间2019/7/29 Logistics, Software, Linear Algebra 主要介绍相关软件以及线性代数的基础知识，由于我使用的设备不支持GPU，所以不考虑使用推荐的软件，关于线性代数的作业也比较基础，并且之前在其它课程中对向量、矩阵等运算已经有了不少训练，因此这一部分暂且略过 Probability and Statistics (Bayes Rule, Sampling Naive Bayes, Sampling) 朴素贝叶斯：课程中以垃圾邮件分类为例大致讲解了朴素贝叶斯的原理，但是非常简略，以至于我没有听懂。。。虽然之前已经学过朴素贝叶斯，但是很久没有接触导致忘的差不多了，下面是根据一篇教程对朴素贝叶斯的一个回顾 假设待分类项$x$具有特征$a_1, a_2, \\cdots, a_n$，（例如对于邮件而言可以把单词是否出现作为特征），$x$可能的分类有$y_1, y_2,\\cdots, y_m$，我们需要对给定的$x$确定最优分类$y_{max}$ 如何利用朴素贝叶斯的原理来解决上面的问题呢？ 首先看一下条件概率的计算 \\begin{align} P(A|B)&=\\frac{P(AB)}{P(B)}\\\\ &=\\frac{P(B|A)P(A)}{P(B)} \\end{align}将$B$看作是特征，$A$看作分类，我们要求的是$P(y=y_i|x=(a_1, a_2, \\cdots, a_n))$，因为对于同一个分类项$x$来说，$P(x=(a_1, a_2, \\cdots, a_n))$是一个常数，因此在计算不同可能分类的条件概率时我们不用考虑它，也就是说$P(y=y_i|x=(a_1, a_2, \\cdots, a_n))\\propto P(x=(a_1, a_2, \\cdots, a_n)|y=y_i)\\cdot P(y_i)$ 当认为各个特征选择完全独立时，$P(x=(a_1, a_2, \\cdots, a_n)|y=y_i)=P(x_1=a_1|y=y_i)\\cdots P(x_n=a_n|y=y_i)$ 即：$P(y=y_i|x=(a_1, a_2, \\cdots, a_n))\\propto P(x_1=a_1|y=y_i)\\cdots P(x_n=a_n|y=y_i)\\cdot P(y_i)$，也是课程中的说明。 有了上面的简要介绍，接下来我们可以针对具体问题来利用朴素贝叶斯来构建分类模型了。与课程一样，这里以MNIST手写数字为例，在MNIST数据集中，$x$可以看作一张图片的所有像素点，每个像素点的取值选择为0/1，$y$总共有十种，从0-9。由此，我们基本对问题定义有了大概的了解，接着需要利用训练集计算$P(x_i=1|y_j)$以及$P(y=y_j)$其中$ i=1～784, j=0～9$，$i$表示像素点的序号，对于测试样例，只要计算$P(x_1=a_1|y=y_j)\\cdots P(x_{784}=a_n|y=y_j)\\cdot P(y_j)$取得最大值的$j$即为预测结果。 具体的分类器构建可以参考STAT 156 Naive Bayes，为了加深对朴素贝叶斯的理解，我自己也构建了一个分类器，Naive Bayes Classifier 在自己实现分类器的过程中，我认为有两点需要注意，其一是$P(x_i=1|y_j)$计算时分母是$y_i$对应的统计数目而非整个训练集大小，其二可以认为是一个小track，在某些情况下$P(x_i=1|y_j)$可能为0，因为需要进行$log$操作，所以这时会报warning，为了避免，可以按照下面的方式初始化： 12xcount = np.ones((10, 28*28))ycount = np.ones((1, 10)) 大数定理&amp;中心极限定理 课程中其它部分主要涉及了概率论的基础知识，介绍了一些概率分布（笔记本中有涉及到），我认为能够掌握大数定理和中心极限定理，对理解课程中的内容有很大帮助，下面简要介绍 大数定理： 根据百度百科 伯努利大数定律：设$\\mu$是n次独立试验中事件A发生的次数，且事件A在每次试验中发生的概率为$p$，则对任意正数ε: $\\lim_{n\\to\\infty}P(|\\frac{\\mu_n}{n}-p|&lt;\\epsilon)=1$ 其含义是，当n足够大时，事件A出现的频率将几乎接近于其发生的概率，即频率的稳定性。 这解释了课程notebook中模拟多次掷骰子的实验结果 中心极限定理：根据《概率论与数理统计》陈希儒版 在概率论中，习惯于把和的分布收敛与正态分布的那一类定理叫做“中心极限定理”，下面的定理就是其中之一： \\begin{align} &设X_1, X_2,\\cdots X_n为独立同分布的随机变量，E(X_i)=a, Var(X_i)=\\sigma^2,0","categories":[],"tags":[{"name":"machine lerning,STAT157","slug":"machine-lerning-STAT157","permalink":"http://yoursite.com/tags/machine-lerning-STAT157/"}]},{"title":"STAT157 L3-L4","slug":"STAT157-L3-L4","date":"2019-07-31T00:57:19.000Z","updated":"2019-07-31T13:27:46.283Z","comments":true,"path":"2019/07/31/STAT157-L3-L4/","link":"","permalink":"http://yoursite.com/2019/07/31/STAT157-L3-L4/","excerpt":"STAT157 L3-L4笔记关于课程STAT157的学习笔记 学习时间2019/7/30 - 2019/7/31","text":"STAT157 L3-L4笔记关于课程STAT157的学习笔记 学习时间2019/7/30 - 2019/7/31 Gradients, Chain Rule, Automatic Differentiation这一节主要讲述梯度的运算技巧以及自动微分 课程内容 梯度运算 课程中关于各种梯度的运算可以总结为下面的一张图 图1:梯度运算总结 按照课程，常数对纵向量的梯度是横向量，这里让我比较疑惑，之前关于梯度的运算我费了很大功夫，虽然没有完全理解，但是基本能计算一些神经网络的反向传播过程，这里我觉得STAT157关于梯度运算讲解的不是很详细，可以参考Derivatives, Backpropagation, and Vectorization以及Vector, Matrix, and Tensor Derivatives 自动微分 自动微分区别于符号微分与数值微分，例如在神经网络的反向传播过程中，我们需要计算损失函数对于各个参数的梯度，然后根据公式来实现代码，这个过程需要一定的数学基础（我现在也只是大概能求解，Jacobian矩阵真的不是很好理解）。在很多深度学习框架中，例如tensorflow，以及本课程用到的MXnet都使用了自动微分技术，以达到向用户隐藏微分的细节。为了更好的说明自动微分，下面没有使用STAT课件中的例子，而是采纳了自动微分(Automatic Differentiation)简介 图2:Autodiff计算图 前向自动微分：图2表示的是一个2输入，1输出的计算图，在前向自动微分中，我们计算每个节点对输入的微分，整个过程从输入到输出，例如$v_0=f_1(x2), v_2=f_2(v1)$，$v_0^{‘}=f_1^{‘}, v_2^{‘}=f_2^{‘}\\cdot v_0^{‘}$ 反向自动微分：与前向自动微分不同，反向自动微分计算的是输出对每个节点的微分，这个过程从输出到输入，与反向传播过程类似。 自动微分(Automatic Differentiation)简介中有更加详细的介绍 Jupyter Notebook 在运行课程notebook时，下面的现象让我有些疑惑 123456x = mx.nd.array([[1, 2], [3, 4]])x.attach_grad()with autograd.record(): y = x * 2 #y.attach_grad() z = y * x 在运行上面的代码时，对于y是否进行attach_grad操作，得到的结果会不同，如果不进行attach_grad操作，x.grad 123[[ 4. 8.] [12. 16.]]&lt;NDArray 2x2 @cpu(0)&gt; 否则为： 123[[2. 4.] [6. 8.]]&lt;NDArray 2x2 @cpu(0)&gt; 猜测是，对y进行attach_grad操作后，计算过程为$\\frac {\\partial z}{\\partial x}=\\frac {\\partial z}{\\partial y}\\cdot\\frac {\\partial y}{\\partial x}$，但是程序计算$\\frac {\\partial z}{\\partial y}=x$，这其实是不正确的，利用符号微分： \\begin{align} \\frac {\\partial z}{\\partial y} &= \\frac {\\partial y}{\\partial y}\\cdot x+\\frac {\\partial x}{\\partial y}\\cdot y\\\\ &=x+\\frac 1 2\\cdot2 x\\\\ &=2x \\end{align}这里给我造成了一些疑惑，课程中并没有进行解释，暂且搁置 关于MXnet Autograd的更多内容，可以参考Autograd Package Linear Regression, Basic Optimization这一小节主要介绍了线性回归模型，并且讲述了如何利用MXnet来进行求解 课程内容 通过预测房价的例子介绍了线性回归模型的定义 介绍了mini-batch SGD 因为线性回归模型比较简单，并且我基本还记得，这里对于课程内容就不做过多记录 Jupyter Notebook 课程中介绍了两种方式实现mini-batch SGD，一种是自己一步一步构建模型，包括前向反馈以及反向传播过程，这与吴恩达老师的神经网络与深度学习中的练习基本类似，不过这里可以利用mxnet的自动微分来进行反向传播过程中的梯度运算，另一种是利用mxnet gluon来搭建模型 在Linear regression中利用上述两种方式完成了模型的构建及优化（数据部分采用课程Notebook） 关于mxnet gluon可以参考Create a neural network","categories":[],"tags":[{"name":"machine lerning","slug":"machine-lerning","permalink":"http://yoursite.com/tags/machine-lerning/"},{"name":"STAT157","slug":"STAT157","permalink":"http://yoursite.com/tags/STAT157/"}]},{"title":"总结—2019/7/27","slug":"总结—2019-7-26","date":"2019-07-28T09:42:06.000Z","updated":"2019-07-28T10:03:43.086Z","comments":true,"path":"2019/07/28/总结—2019-7-26/","link":"","permalink":"http://yoursite.com/2019/07/28/总结—2019-7-26/","excerpt":"总结-2019/7/27下面主要对7-22～7-26这几天学习的一些知识进行一次回顾","text":"总结-2019/7/27下面主要对7-22～7-26这几天学习的一些知识进行一次回顾 主要学习资料 吴恩达coursera课程 Derivatives, Backpropagation, and Vectorization Xavier and He Normal (He-et-al) Initialization 内容回顾part1：手工构建神经网络​ 在这一周，通过课程我学习了如何利用numpy等python库来搭建一个多层的神经网络，并且对其进行训练。多层神经网络的结构如下图所示： 图1:多层神经网络结构 ​ 为了构建这样一个多层的神经网络，大致需要经过以下几个步骤 参数的初始化：例如从输入到隐藏层，我们需要的是权值矩阵以及偏置，这在一开始就需要进行初始化。一般来说，初始化所有的参数，需要先定义出各层的规模。参数的初始化对之后的训练会起到一定影响，具体会在之后说明。 前向反馈过程：有了参数以及输入，我们需要实现上述神经网络的计算过程。以图一为例（参考其网络结构，但是更改输入为纵向量），整个网络的计算过程如下： Z_1 = W_1*X+b_1,\\ \\ \\ \\ \\ A_1 = f_1(Z_1)\\\\ Z_2 = W_2*A_1+b_2, \\ \\ \\ \\ A_2 = f_2(Z_2)\\\\ Z_3 = W_o*A_2+b_3,\\ \\ \\ \\ \\ \\ \\hat Y = f_3(Z_3)其中$f$表示激活函数，常见的有$relu, sigmoid$等，最后为了得到输出，一般会使用$sigmoid$或$softmax$，具体需要看是二分类还是多分类问题。 计算损失函数：上述神经网络最后可以计算得到预测结果$\\hat y$，以二分类问题为例，对数似然损失函数如下： loss = -[y*log(\\hat y)\\ +\\ (1-y)*log(1-\\hat y)]上面展示的是对一条数据的计算过程，当进行向量化处理时需要求均值。 反向传播过程：利用反向传播，我们可以计算出损失函数对各个参数的导数，之后对各参数进行更新以降低模型在训练数据上的损失，假设训练数据的规模为$m$，并且输出层为$sigmoid$，那么图1的反向传播可以表示如下： loss = -1/m*[Y*log(\\hat Y)\\ +\\ (1-Y)*log(1-\\hat Y)]\\\\ dZ_3 = \\hat Y\\ -\\ Y\\\\ \\ \\\\ dW_o = 1/m*dZ_3*A_2.T,\\ db_3 = 1/m*np.sum(dZ_3, axis=1)\\\\ dA_2 = W_o.T*dZ_3,\\ dZ_2 = dA_2*f_2^{'}(Z_2)\\\\ \\ \\\\ dW_2 = 1/m*dZ_2*A_1.T,\\ db_2 = 1/m*np.sum(dZ_2, axis=1)\\\\ dA_1 = W_2.T*dZ_2,\\ dZ_1 = dA_1*f_1^{'}(Z_1)\\\\ \\ \\\\ dW_1 = 1/m*dZ_1*A_1.T,\\ db_1 = 1/m*np.sum(dZ_1, axis=1) 模型的组合：利用上述的几个部分，可以定义出一个神经网络模型，在训练集上进行多次迭代，不断更新参数以降低损失函数，最后得到模型的参数 预测：利用学习得到的参数以及前向反馈过程就可以对测试集进行预测 part2：参数初始化在part1中已经提到模型的参数初始化会对模型训练产生影响，下面就对参数初始化进行具体介绍。 Zero Initialization 将所有的权值矩阵初始化为0，假设中间层的激活函数是$relu$，输出层的激活函数为$sigmoid$ \\begin{align} dW_1 &= dZ_1*X.T \\\\ &=dA_1*f^{'}(Z_1)*X.T\\\\ &=W_2.T*dZ_2*f^{'}(Z_1)*X.T \\end{align}可以看到在第一次迭代当中，$dW_1$受到$W_2$的影响结果为0，因此不会对$W_1$更新，类似的其它权值矩阵相同。对于$dW_L$: \\begin{align} dW_L &=dZ_L*A_{L-1}.T\\\\ &=(A_L-Y)*A_{L-1}.T \\end{align}因为中间层激活函数是$relu$，因此$dW_L$为0。如果激活函数是$tanh$，那么在第一轮迭代后会对$W_L$更新。但是由于权值矩阵中的每一行都是相同的（行向量可以看作是对输入的一个映射），根据对称性，在同一层的神经元作用相同，因此完全不推荐使用Zero Initialization Random Initialization 为了使每个神经元发挥不同的作用，我们需要保证权值矩阵中的每行向量具有区分度，因此我们可以利用Random Initialization来进行初始化。利用np.random.randn可以得到服从$N(\\mu=0, \\delta^2=1)$的特定$shape$的矩阵。但是初始化时权值矩阵的方差选择会成为比较棘手的问题，当选择过小时可能会出现梯度消失，导致训练速度过慢，当选择过大时可能会出现梯度爆炸，导致无法训练。 HE Normal Initialization 下面以中间层激活函数为$relu$的神经网络作为说明 \\begin{align} Z_1 &= W_1*X\\\\ A_1 &= relu(Z_1)\\\\ Z_{1_{ij}} &= \\sum_{k=0}^{k=nc}W_{ik}\\cdot X_{kj} \\end{align}$nc$表示$X$的特征数，或者$W_1$的列数。其中假设$W_1\\sim N(0, 1), X\\sim N(0, 1)$，利用基础的概率轮知识可以得到$Z_1\\sim N(0, nc)$，也就是说经过$W_1$后$Z_1$的方差变大了，为了尽可能优化后面的反向传播过程，这里将$W_1$的方差设置为$2/nc$，分子为2是因为采用的是$relu$函数，这种初始化的方法叫做：HE Normal Initialization [ ] Q2.1:关于这里的初始化我有一些不理解的地方，比如激活函数为$relu$时为什么分子选择2，$tanh$时选择分子为1？从$Z_1$到$A_1$，由于经过了一个非线性的映射，所以输出的分布完全变化了，在这时是否还是根据的方差或者均值来选择的分子呢？要解决这个疑问，后续可能需要阅读提出He-et-al的论文。 part3：正则化在机器学习的过程中，我们最终需要的是模型在测试集上取得优异的表现，但是在有些时候，模型可能会发生过拟合的情况，例如下图： 图2：Over-fitting $L_2$正则化 $loss = 1/m\\cdot \\zeta(\\hat Y,Y)+\\frac{\\lambda}{2m}\\cdot||\\theta||^2$ $\\frac{\\lambda}{2m}\\cdot||\\theta||^2$表示正则化项，将损失函数加上$L_2$正则化项后可以避免权值矩阵过大，以此可以降低过拟合 Dropout dropout，即随机失活，意味着在神经网络中随机对一些神经元进行失活操作，具体的操作需要一个失活矩阵$D$来实现： \\begin{align} Z_i &= W_i*A_{i-1}\\\\ A_i &= f(Z_i)\\\\ A_i &= A_i\\cdot D_i/keep_prob \\end{align}其中矩阵$D_i$是根据$keep_prob$来确定的0/1矩阵 [ ] Q3.1：$L_2$正则化有利于降低权值矩阵大小，但是权值矩阵的绝对大小对整个神经网络的效果会有什么样的影响呢？如何用数学化的形式具体说明$L_2$正则化是如何降低过拟合的呢？ [ ] Q3.2：在课程中，dropout是在激活层之后使用的，如果在激活层之前dropout会有什么区别呢？ [ ] Q3.3：dropout后保持输出的均值相同有什么作用 (利用$/keep_prob$保持均值) [ ] Q3.4：dropout是如何降低过拟合的？ part4：神经网络的优化Gradient descent，也就是梯度下降，是课程中一开始就使用的优化算法。根据维基百科: 梯度下降方法基于以下的观察：如果实值函数$\\displaystyle F(\\mathbf {x} )$在点$\\displaystyle \\mathbf {a} $处可微且有定义，那么函数$\\displaystyle F(\\mathbf {x} )$在$\\displaystyle \\mathbf {a} $点沿着梯度相反的方向 $\\displaystyle -\\nabla F(\\mathbf {a} )$ 下降最快。 因此，梯度下降通常也称为最速下降法。但是在数据规模很大时，梯度下降的成本非常高，因此需要进行一些转变 Mini-batch graillent decent: 小批量梯度下降，假设$m$是整个训练集的大小，$m_{mini}$表示一个小批量数据集的大小，那么在小批量梯度下降中，算法每一次会利用$m_{mini}$条数据进行参数的更新： \\begin{align} loss^t = 1/m_{mini}\\cdot\\zeta(\\hat Y^t, Y^t) \\end{align}$t$表示批次 小批量梯度下降算法每次都会优化$loss^t$，但是对于整个训练集而已，$loss$并不一定呈现出优化的状态 图3: 损失函数 Momentum(动量) gradient decent：在梯度下降中，我们是利用当前的$dW$来更新权值矩阵，momentum梯度下降则不同，它利用指数移动加权平均的思想，通过计算一个加权的$V_{dW}$来对权值矩阵进行更新： \\begin{align} V_{dW} &= 0\\\\ V_{dW_1} &= \\beta V_{dW}+(1-\\beta)dW_1\\\\ &\\cdots\\\\ V_{dW_k} &= \\beta V_{dW_{k-1}}+(1-\\beta)dW_k \\end{align} RMSprop(均方根传递)：调整$dW$大小来加速优化过程： \\begin{align} S_{dW} &= 0\\\\ S_{dW_1} &= \\beta S_{dW}+(1-\\beta)dW_1^2\\\\ &\\cdots\\\\ S_{dW_k} &= \\beta S_{dW{k-1}}+(1-\\beta)dW_{k-1}^2 \\end{align}$W$的更新如下：$W = W-\\alpha\\frac {dW}{\\sqrt {S_{dW}}}$ Adam：Adam算法融合了上面两种算法 \\begin{align} &V_{dW_k} = \\beta_1V_{d_{W_{k-1}}} +(1-\\beta_1)dW_k\\\\ &S_{dW_k} = \\beta_2 S_{dW{k-1}}+(1-\\beta_2)dW_{k-1}^2\\\\ &V_{dW_kcorr}=\\frac{V_{dW_k}}{1-\\beta_1^t}\\\\ &S_{dW_kcorr}=\\frac{S_{dW_k}}{1-\\beta_2^t}\\\\ \\end{align}$W$的更新如下：$W = W-\\alpha\\frac {V_{dW_kcorr}}{\\sqrt {S_{dW_kcorr}}}$ [ ] Q4.1：为什么mini-batch gradient dencent可以降低整个数据集上的损失函数？ [ ] Q4.2：mini-batch gradient dencent的噪声是由哪些因素决定的？这些因素又是怎样影响到噪声大小的？ [ ] Q4.3：上面提到的动量、RMSprop以及Adam加速优化的原因？","categories":[],"tags":[{"name":"machien learning","slug":"machien-learning","permalink":"http://yoursite.com/tags/machien-learning/"}]},{"title":"numpy向量化操作","slug":"numpy向量化操作","date":"2019-06-29T00:48:14.000Z","updated":"2019-06-29T01:03:07.153Z","comments":true,"path":"2019/06/29/numpy向量化操作/","link":"","permalink":"http://yoursite.com/2019/06/29/numpy向量化操作/","excerpt":"numpy向量化操作本文记录一些常见的利用numpy向量化操作提高运行速度的方法","text":"numpy向量化操作本文记录一些常见的利用numpy向量化操作提高运行速度的方法 0\\1矩阵的生成 在机器学习中，如果是二分类问题，最后需要将概率矩阵转化为0/1矩阵，如果使用for循环就没有办法利用到numpy的优势 下面是几个例子 1234567891011121314def get_ypre(A, dim): y_pre = np.zeros((1, dim)) for i in range(dim): if A[0, i] &gt; 0.5: y_pre[0, i] = 1 return y_predef get_ypre_vectorize_1(A, dim): y_pre = np.where(A &lt; 0.5, 0, 1) return y_pre def get_ypre_vectorize_2(A, dim): y_pre = np.floor(A + 0.5) return y_pre get_ypre函数利用for循环遍历需要转换为0/1矩阵的A矩阵，get_ypre_vectorize_1和get_ypre_vectorize_2分别利用numpy的np.where和np.floor进行向量化处理，运行时间如下所示 12345678dim = 10**7A = np.random.rand(1, dim)timeit(lambda:get_ypre(A, dim), number=1)out : 2.667440585035365timeit(lambda:get_ypre_vectorize_1(A, dim), number=1)out : 0.06458032497903332timeit(lambda:get_ypre_vectorize_1(A, dim), number=1)out : 0.06309162796242163 可以看到提升效果比较明显","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"sklearn common usage","slug":"sklearn-common-usage","date":"2019-06-23T01:41:02.000Z","updated":"2019-06-24T07:11:33.899Z","comments":true,"path":"2019/06/23/sklearn-common-usage/","link":"","permalink":"http://yoursite.com/2019/06/23/sklearn-common-usage/","excerpt":"","text":"sklearn common usage本文主要记录使用sklearn中遇到的一些常见用法 scikit-learn Preprocessing SimpleImputer 123from sklearn.impute import SimpleImputerimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')imp_mean.fit_transform([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]]) OneHotEncoder&amp;LabelEncoder 1234from sklearn.preprocessing import OneHotEncoderenc = OneHotEncoder(handle_unknown=&apos;ignore&apos;)X = [[&apos;Male&apos;, 1], [&apos;Female&apos;, 3], [&apos;Female&apos;, 2]]enc.fit_transform(X) 123from sklearn.preprocessing import LabelEncoderle = LabelEncoder()le.fit_transform([1, 2, 2, 6]) 一般在使用onehot之前先使用labelencoder进行处理，见例子1,例子2 Model selection and evaluation train_test_split 12from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) StandardScaler The standard score of a sample x is calculated as: z = (x - u) / s 123from sklearn.preprocessing import StandardScalerscaler = StandardScaler()scaler.fit_transform(X) Regression LinearRegression 123from sklearn.linear_model import LinearRegressionreg = LinearRegression().fit(X, y)reg.predict(np.array([[3, 5]]))","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"Stationarity of time series","slug":"Stationarity-of-time-series","date":"2019-06-15T02:22:40.000Z","updated":"2019-06-15T03:01:26.275Z","comments":true,"path":"2019/06/15/Stationarity-of-time-series/","link":"","permalink":"http://yoursite.com/2019/06/15/Stationarity-of-time-series/","excerpt":"","text":"Stationarity of time series本文主要记录学习过程中关于时间序列平稳性的一些相关内容，包括平稳性的定义以及检测 平稳性定义 平稳性 在实际应用过程中，我们一般通过弱平稳来检测平稳性 上图来自知乎 平稳性检测123456789101112131415161718192021222324from statsmodels.tsa.stattools import adfullerdef test_stationarity(timeseries): #Determing rolling statistics rolmean = timeseries.rolling(12).mean() rolstd = timeseries.rolling(12).std() #Plot rolling statistics: orig = plt.plot(timeseries, color='blue',label='Original') mean = plt.plot(rolmean, color='red', label='Rolling Mean') std = plt.plot(rolstd, color='black', label = 'Rolling Std') plt.legend(loc='best') plt.title('Rolling Mean &amp; Standard Deviation') plt.show(block=False) #Perform Dickey-Fuller test: print ('Results of Dickey-Fuller Test:') dftest = adfuller(timeseries, autolag='AIC') dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used']) for key,value in dftest[4].items(): dfoutput['Critical Value (%s)'%key] = value print (dfoutput)test_stationarity(ts) 平稳性检测 利用adfuller检测时间序列平稳性，当Test Statistic小于某个Critical Value时则认为在该置信下认为时间序列平稳 时间序列平稳化差分&amp;去除趋势","categories":[],"tags":[{"name":"Statics","slug":"Statics","permalink":"http://yoursite.com/tags/Statics/"}]},{"title":"pandas common usage","slug":"pandas-common-usage","date":"2019-06-10T11:24:30.000Z","updated":"2019-06-29T01:07:32.697Z","comments":true,"path":"2019/06/10/pandas-common-usage/","link":"","permalink":"http://yoursite.com/2019/06/10/pandas-common-usage/","excerpt":"","text":"pandas common usage pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. pandas 本文主要记录在实践中遇到的一些常见的用法 一些说明 df表示pandas.DataFrame dataFrame rolling 为了提升数据的准确性，将某个点的取值扩大到包含这个点的一段区间，用区间来进行判断，这个区间就是窗口。移动窗口就是窗口向一端滑行，默认是从右往左 rolling返回的类可以进行很多数值操作，例如mean(), std(), sum(),这些均是dataFrame包含的方法 dropna 去除空值 Merge, join, and concatenate 对多个dataframe进行合并操作，包括行、列数据 duplicated 判断重复数据 drop_duplicates ​ 去除重复数据 group_by 根据某些col对数据进行聚合 as_matrix 转化为numpy的数组，注意columns参数是列名的列表 columns 获取列名 获取某个位置的元素 DataFrame.iat Fast integer location scalar accessor. DataFrame.loc Purely label-location based indexer for selection by label. Series.iloc Purely integer-location based indexing for selection by position. values 返回numpy 数组值 apply) 123df.apply(np.sum, axis=0)df.apply(lambda x: [1, 2], axis=1)X[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col))","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Josephus Problem","slug":"Josephus-Problem","date":"2019-01-05T06:42:32.000Z","updated":"2019-01-12T03:13:33.218Z","comments":true,"path":"2019/01/05/Josephus-Problem/","link":"","permalink":"http://yoursite.com/2019/01/05/Josephus-Problem/","excerpt":"","text":"Josephus Problem问题引入 In computer science and mathematics, the Josephus problem (or Josephus permutation) is a theoretical problem related to a certain counting-out game. People are standing in a circle waiting to be executed. Counting begins at a specified point in the circle and proceeds around the circle in a specified direction. After a specified number of people are skipped, the next person is executed. The procedure is repeated with the remaining people, starting with the next person, going in the same direction and skipping the same number of people, until only one person remains, and is freed. The problem — given the number of people, starting point, direction, and number to be skipped — is to choose the position in the initial circle to avoid execution more info 约瑟夫是公元一世纪著名的历史学家。在罗马人占领乔塔帕特后，39 个犹太人与约瑟夫及他的朋友躲到一个洞中，39个犹太人决定宁愿死也不要被敌人俘虏，于是决定了一个流传千古的自杀方式，41个人排成一个圆圈，由第1个人开始报数，每报到第3人该人就必须自杀，然后再由下一个人重新报数，直到所有人都自杀身亡为止。 Josephus problem是一个非常有意思的问题，理解起来很简单，但是要解决却不是太容易。以前在程序课上接触过这个问题，当时的解决方法非常暴力，直接模拟整个游戏过程来得到最终答案。最近读到《具体数学》再次接触到了这个问题，书中的解法确有让人拍案而起的欲望。下面从该问题最基础的类型说起，一步一步解释书中的解法。 基础问题 现有一个n节点的圆圈，且从1到n有序排列。从1开始计数，每隔一个节点去除一个节点，问最后剩下的节点（下面简称幸存者）序号是多少？ Figure 1 要解决这个问题，可以从递归的角度来考虑，这一点在图1中有很直观的表达。针对我们的问题，我们要求解的是幸存者序号，不妨表示为$f(n)$，根据n的奇偶性可以得到下面的关系 \\begin{align} 问题1 \\begin{cases} f(1) &=& 1\\\\ f(2n) &=& 2·f(n)-1\\\\ f(2n+1) &=& 2·f(n)+1\\\\ \\end{cases} \\end{align} 现在的问题是如何根据上面的关系求出$f(n)$的表达式。首先不妨从一些简单的例子观察一下规律 n 1 2 3 4 5 6 7 8 $f$ 1 1 2 1 2 3 4 1 看起来是有一定规律的，写成下面这样也许更清晰 n $2^0$ $2^1$ $2^1+1$ $2^2$ $2^2+1$ $2^2+2$ $2^2+3$ $2^3$ $f$ 1 1 3 1 3 5 7 1 简单的归纳就是$f(n) = 2l+1, n=2^m+l$，下面用数学归纳法来证明这一结论 \\begin{align} &需要证明：对于n=2^m+l, f(n)=2l+1,其中0=","categories":[],"tags":[{"name":"Mathematics","slug":"Mathematics","permalink":"http://yoursite.com/tags/Mathematics/"},{"name":"Concrete Mathematics","slug":"Concrete-Mathematics","permalink":"http://yoursite.com/tags/Concrete-Mathematics/"},{"name":"Recurrent Problem","slug":"Recurrent-Problem","permalink":"http://yoursite.com/tags/Recurrent-Problem/"}]},{"title":"BurnSide's Lemma","slug":"BurnSide-Lemma","date":"2018-12-30T00:35:41.000Z","updated":"2019-01-05T08:01:32.631Z","comments":true,"path":"2018/12/30/BurnSide-Lemma/","link":"","permalink":"http://yoursite.com/2018/12/30/BurnSide-Lemma/","excerpt":"","text":"Burnside’s lemma问题引入 利用三种颜色给一个正三边形的顶点着色，在考虑图案重复的情况下总共有多少不同的着色方法？ 上面的着色问题是一类计数问题，在计数的过程中由于需要考虑到去除重复的元素，所以问题会显得比较复杂 Figure 1 以上图为例，六种三角形的着色实际是等价的，只能算一种着色方式。 Burnside’s lemma定义 Burnside’s lemma, sometimes also called Burnside’s counting theorem, the Cauchy–Frobenius lemma,orbit-counting theorem, or The Lemma that is not Burnside’s , is a result in group theory which is often useful in taking account of symmetry when counting mathematical objects.In the following, let G be a finite group) that acts on a set) X. For each g in G let Xg denote the set of elements) in X that are fixed by) g (also said to be left invariant) by g), i.e. $X^g $ = { x ∈ X | g.x = x }. Burnside’s lemma asserts the following formula for the number of orbits), denoted |X/G| $$|X/G|={\\frac {1}{|G|}}\\sum _{{g\\in G}}|X^{g}|$$ Burnside’s lemma 上面给出了Burnside’s lemma的定义，其具体的证明过程可参考给出的链接。利用Burnside’s lemma我们现在可以来试着解决一开始给出的着色问题了。 问题解决 着色问题的形式化定义 $X :=\\{不考虑重复的所有着色图案\\}$ $G :=\\{对任一图案的所有变换\\}$ $|X/G| := 所有不重复的着色方式总数$ $G$ 代表了对图案的变换方式，图1实际上就包含了对三角形的所有变换方式。其中旋转变换有三种，对称变换有三种。以旋转$\\frac 2 3 \\pi$为例，$X^g = \\{x | colorA = colorB = colorC\\}$ 只有满足条件的着色图案才有$g.x=x$ ，不难计算这类着色方式有三种，$3^1$ 对于上述的三色三角形问题，总共有 $\\frac 1 6(3^3+2·3^1+3·3^2) = 10$种着手方式 问题一般化 利用上述定理可以解决任意$(n, m)$着色问题，其中n代表正多边形的顶点数，m代表颜色数。 question Figure 2 图2便是一个一般化的正多边形的着色问题。 解决这个一般化问题可以从两个部分入手 旋转变换对应的$X^g$: 对于n边形，旋转变换共有n种，旋转度数从$0-2\\pi$,关键问题是如何计算每一种旋转变换的着色数量。下面尝试将问题形式化 \\begin{align} &对于任一旋转变换d(0=< d < n),顶点可分为以下几类\\\\ &\\Phi _0 = \\{0, d, 2d, ...\\}\\\\ &\\Phi _1 = \\{1, d+1, 2d+1, ...\\}\\\\ &...\\\\ &\\Phi _i = \\{i, d+i, 2d+i, ...\\}\\\\ &对于任一顶点x, x属于并仅属于某一\\Phi\\\\ &对任一(x_1,x_2)属于\\Phi_j, x_1 = x_2+kd\\ (mod\\ n)\\\\ &对于同一\\Phi中的顶点，颜色应该一致，总的着色数有m^{i+1} \\end{align} 对称变换较容易，考虑奇偶分类处理即可 Python代码 1234567891011121314151617181920212223for step in range(n): count = &#123;&#125; for i in range(n): count[i] = 0 uncount, ucNow, ucAll = True, 0, [] while True: ucFirst, orbitSet = 0, [] while ucFirst != n: if count[ucFirst] == 0: break else: ucFirst += 1 if ucFirst == n: break i = ucFirst while count[i] != 1: orbitSet.append(i) count[i] = 1 i = (i+step)%n ucNow += 1//模拟得出所有的orbitSet集合，例如n=10，step=2时，orbitSet集合为[0, 2, 4, 6, 8][1, 3, 5, 7, 9] 通过证明可以得到$|\\Phi| = grd(n, d)$, 这样就不用计算每个集合的元素，直接可以得到集合个数 对称变换太简单，这里不详细说明。 其它类似问题","categories":[],"tags":[{"name":"Mathematics","slug":"Mathematics","permalink":"http://yoursite.com/tags/Mathematics/"},{"name":"GroupTheory","slug":"GroupTheory","permalink":"http://yoursite.com/tags/GroupTheory/"},{"name":"CountProblem","slug":"CountProblem","permalink":"http://yoursite.com/tags/CountProblem/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://yoursite.com/tags/Algorithm/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]}]}